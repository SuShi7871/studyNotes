# 爬虫

## Urllib

```python
# 使用urllib来获取百度首页的源码
import urllib.request

# 定义一个url  就是你要访问的地址
url = 'http://www.baidu.com'

# 模拟浏览器向服务器发送请求 response响应
response = urllib.request.urlopen(url)

# 获取响应中的页面的源码  content 内容的意思
# read方法  返回的是字节形式的二进制数据
# 我们要将二进制的数据转换为字符串
# 二进制--》字符串  解码  decode('编码的格式')
content = response.read().decode('utf-8')

# 获取response的返回值类型，response是HTTPResponse的类型
print(type(response))

# 按照一个字节一个字节的去读
content = response.read()
# 返回多少个字节
content = response.read(5)
# 读取一行
content = response.readline()
# 按行读取，直至结束
content = response.readlines()

# 返回状态码、url、其他信息
response.getcode()
response.geturl()
response.getHeaders()
```

### 下载文件

```python
url_page = 'http://www.baidu.com'
# 爬取网页，返回值是一个网页urllib.request.urlretrieve(url_page,'baidu.html')

# 下载图片，返回值是一张图片
url_img = '图片地址'
urllib.request.urlretrieve(url=url_img,filename='james.jpg')

# 下载视频
url_video = '视频地址'
urllib.request.urlretrieve(url_video,'hxekyyds.mp4')
```

### 请求对象的定制

User Agent中文名为用户代理，简称 UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本。浏览器内核、浏览器渲染引擎、浏览器语言、浏览器插件等

一条url有以下部分组成:协议 、主机、 端口号、路径、参数、锚点

协议例如http以及https

HTTP是一种用于从网络传输超文本到本地浏览器的协议，它使得用户能够访问互联网上的信息。他是明文传输即HTTP数据在传输过程中不加密，因此容易被截获和篡改。他是无状态的即HTTP协议本身不保存之前客户端和服务器的交互状态。

HTTPS是HTTP的安全版本，它在HTTP的基础上通过SSL/TLS协议提供了数据加密、完整性校验和身份验证。

- HTTPS比HTTP更安全，因为它提供了数据加密和完整性保护。
- 由于加密和解密的需要，HTTPS可能会比HTTP稍微慢一些，但这种差异通常很小。
- HTTP通常用于普通的网页浏览，而HTTPS用于需要安全通信的场景，如网上银行、在线购物、登录页面等。

```python
import urllib.request
url = 'https://www.baidu.com'
"""
创建一个字典headers，包含请求头信息。这里模拟了一个常见的浏览器用户代理字符串，以避免被服务器识别为爬虫。
"""
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}
# 因为urlopen方法中不能存储字典 所以headers不能传递进去,所以需要请求对象的定制
request = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf8')
print(content)
```

### 编解码

#### get请求:quote

我们平时从浏览器中的导航栏获取url的时候，粘贴到其他地方会出现有些字符被解析为unicode编码，不方便识别所以我们需要做的事情就是将其恢复成可识别的字符。比如一下例子：

浏览器中看到的

`https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=詹姆斯`

粘贴出来的效果

`https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=%E8%A9%B9%E5%A7%86%E6%96%AF`

```python
# 需求 获取 https://www.baidu.com/s?wd=詹姆斯的网页源码
import urllib.request
import urllib.parse
url = 'https://www.baidu.com/s?wd='
# 请求对象的定制为了解决反爬的第一种手段
headers = {
    'User-Agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) '
'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 '
'Mobile Safari/537.36Move tracks around with updated UI '
'in Performance panelChange the order of tracks and hide '
'them with an improved configuration mode in the Performance '
'panel.Ignore scripts in the flame chartAdd and remove scripts'
' to and from the ignore list right in the flame chart.Find '
'excessive memory usage with filters in heap snapshotsFind common '
'cases of inefficient memory usage using new filters in heap snapshots.'
}

# 将詹姆斯三个字变成unicode编码的格式
# 我们需要依赖于urllib.parse
name = urllib.parse.quote('詹姆斯')
url = url + name

# 请求对象的定制
request = urllib.request.Request(url=url,headers=headers)

# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(request)

# 获取响应的内容
content = response.read().decode('utf-8')

# 打印数据
print(content)
```

#### get请求：urlencode

如果url后面携带的参数有多个，那么解析的时候就需要使用`urlencode`来解析多参数

```python
import urllib.request
import urllib.parse
base_url = 'https://www.baidu.com/s?'
data = {    
    'rsv_idx': 1,    
    'wd': '詹姆斯的个人资料简介',    
    'base_query': '詹姆斯',    
    'tag_key': '个人资料简介'
}
new_data = urllib.parse.urlencode(data)
# 请求资源路径
url = base_url + new_data
headers = {    'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) 				AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Mobile 		  				 Safari/537.36'
          }
request = urllib.request.Request(url=url, headers=headers)
# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(request)
# 获取响应的内容
content = response.read().decode('utf-8')
# 打印数据print(content)
```

#### post请求方式

post和get区别：

- get请求方式的参数必须编码，参数是拼接到url后面，编码之后不需要调用encode方法
- post请求方式的参数必须编码，参数是放在请求对象定制的方法中，编码之后需要调用encode方法

```python
import urllib.request
import  urllib.parse
import json
# 这个url需要去进入到控制台去寻找接口
url = "https://fanyi.baidu.com/sug"
headers = {    'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) 				 AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Mobile 	  				  Safari/537.36'
          }
data = {    
    'kw':'tom'
}
# post请求的参数 必须要进行编码
data = urllib.parse.urlencode(data).encode('utf-8')
# post的请求的参数 是不会拼接在url的后面的而是需要放在请求对象定制的参数中
# post请求的参数 必须要进行编码
request = urllib.request.Request(url=url,data=data,headers=headers)
# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(request)
# 获取响应的数据
content = response.read().decode('utf-8')
print(content)
# 将字符串转换为json对象
obj = json.loads(content)print(obj)
```

#### URLError和HTTPError的区别

-  `URLError` 是由 `urllib.error` 模块提供的异常类。当网络请求过程中出现了与 URL 相关的错误（如无法连接到服务器，DNS 查找失败等），会抛出 `URLError`。`URLError` 对象包含一个 `reason` 属性，表示导致错误的原因。

-  `HTTPError` 是 `URLError` 的子类，也是由 `urllib.error` 模块提供的异常类，当 HTTP 请求返回错误的 HTTP 状态码（如 404, 500 等）时，会抛出 `HTTPError`。`HTTPError` 对象不仅包含 `URLError` 的 `reason` 属性，还包含 `code`（HTTP 状态码）和 `headers`（响应头）属性。


### cookie登录





