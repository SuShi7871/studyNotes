---

---

# 深度学习

## 数据操作

### 入门

```python
import torchx = torch.arange(12)
print(x.numel()) # 张量中元素的总数
print(x.shape) # 可以通过张量的shape属性来访问张量(沿每个轴的长度)的形状。
X = x.reshape(3,4) # 改变一个张量的形状而不改变元素数量和元素值，可以调用reshape函数。
print(X)

"""
    我们不需要通过手动指定每个维度来改变形状,我们可以通过-1来调用此自动计算出维度的功能。即我们可以
    用x.reshape(-1,4)或x.reshape(3,-1)来取代x.reshape(3,4)。
"""
Y = x.reshape(-1,4)
Z = x.reshape(3,-1)
print(Y)
print(Z)

# 创建一个形状为(2,3,4)的张量，其中所有元素都设置为0
zero = torch.zeros((2, 3, 4))
print(zero)
# 我们可以创建一个形状为(2,3,4)的张量，其中所有元素都设置为1
one = torch.ones(2,3,4)
print(one)
# 创建一个形状为(3,4)的张量。其中的每个元素都从均值为0、标准差为1的标准高斯分布(正态分布)中随机采样。
gaosi = torch.randn(3, 4)
print(gaosi)
"""
通过提供包含数值的Python列表(或嵌套列表)，来为所需张量中的每个元素赋予确定值。在这
里，最外层的列表对应于轴0，内层的列表对应于轴1。
"""
confirmVal =torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print(confirmVal)
```

###  运算符

可以把多个张量连结在一起，把它们端对端地叠起来形成一个更大的张量。我们只需要提供张量列表，并给出沿哪个轴连结。

```python
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print(X)print(Y)print(torch.cat((X, Y), dim=0)) # 按列链接，x方向
print( torch.cat((X, Y), dim=1)) # 按行链接，y方向
```

### 广播机制

在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制来执行按元素操作。

这种机制的工作方式如下：

1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
2. 对生成的数组执行按元素操作。

```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
"""
由于a和b分别是3 × 1和1 × 2矩阵，如果让它们相加，它们的形状不匹配。我们将两个矩阵广播为一个更大的3 × 2矩阵，如下所示：矩阵a将复制列，矩阵b将复制行，然后再按元素相加。
"""
```

与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是‐1；可以指定范围以包含第一个元素和最后一个之前的元素。如下所示，我们可以用[-1]选择最后一个元素，可以用`[1:3]`选择第二个和第三个元素：

```python
X[-1], X[1:3]
# [0:2, :]访问第1行和第2行，其中“:”代表沿轴1(列)的所有元素。
X[0:2, :] = 12print(X)
"""tensor([[12, 12, 12, 12],      
			[12, 12, 12, 12],       
			[ 8,  9, 10, 11]])
"""
```

### 线性代数

####  降维&点积

默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。我们还可以指定张量沿哪一
个轴来通过求和降低维度。

```python
A_sum_axis0 = A.sum(axis=0) # 求和所有行的元素来降维(轴0)
print(A_sum_axis0, A_sum_axis0.shape) # tensor([40, 45, 50, 55]) torch.Size([4])
A_sum_axis1 = A.sum(axis=1) # 求和所有列的元素来降维(轴1)
print(A_sum_axis1, A_sum_axis1.shape) # tensor([ 6, 22, 38, 54, 70]) torch.Size([5])
print(A.sum(axis=[0, 1]) )# 结果和A.sum()相同
```

如果我们想沿某个轴计算A元素的累积总和，比如axis=0(按行计算)，可以调用`cumsum`函数。此函数不会沿任何轴降低输入张量的维度。

```python
x = torch.arange(4,dtype=torch.float32)
y = torch.ones(4, dtype = torch.float32)
print(torch.dot(x,y)) # 6
```

#### 矩阵-向量积&矩阵乘法

在代码中使用张量表示矩阵‐向量积，我们使用`mv`函数。当我们为矩阵A和向量x调用`torch.mv(A, x)`时，会执行矩阵‐向量积。注意，A的列维数(沿轴1的长度)必须与x的维数(其长度)相同。

```python
A.shape, x.shape, torch.mv(A, x)
B = torch.ones(4, 3)
C = torch.ones(3,2)
print(torch.mm(B,C)) # 矩阵乘法
```

## 线性回归

### SoftMax回归

独热编码。独热编码是一个向量，它的分量和类别一样多。类别对应的分量设置为1，其他所有分量设置为0。

![1715740557255](D:\software\Typora\typora-user-images\1715740557255.png)

与线性回归一样，`softmax`回归也是一个单层神经网络。由于计算每个输出`o1、o2和o3`取决于所有输入`x1、x2、x3和x4`，所以**`softmax`回归的输出层也是全连接层**。

我们希望模型的输出$y_j$可以视为属于类j的概率，例如，如果$\hat{y}_1$、$\hat{y}_2$和$\hat{y}_3$分别为0.1、0.8和0.1，那么我们预测的类别是2，在我们的例子中代表“鸡”。

**`softmax`函数能够将未规范化的预测变换为非负数并且总和为1**，同时让模型保持可导的性质。为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：
$$
y^i=softmax(o)其中 y^j =\frac{exp(o_j)}{ \sum{}{}_kexp(o_k)}
$$
对于所有的j总有$0≤y^j ≤1$。因此，$y^j$可以视为一个正确的概率分布。尽管`softmax`是一个非线性函数，但`softmax`回归的输出仍然由输入特征的仿射变换决定。因此，`softmax`回归是一个线性模型

#### 损失函数

$$
l(y,y^i)=-\sum_{j=1}^{q}y_ilog\hat{y}_i
$$

上面的损失函数通常被称为交叉熵损失，它用于衡量实际输出分布与期望输出分布(或者说是预测分布与真实分布)之间的差异。在具体的形式上，交叉熵损失函数通常用来计算分类问题中的二元分类或多分类问题中的损失。

## 多层感知机

我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，**使其能处理更普遍的函数关系类型**。要做到这一点，最简单的方法是将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前L−1层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机`(multilayer perceptron)`，通常缩写为`MLP`。

![1715743918372](D:\software\Typora\typora-user-images\1715743918372.png)

 一个单隐藏层的多层感知机，具有5个隐藏单元。输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。因此，这个多层感知机中的层数为2。注意，这两个层都是全连接的。每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。

我们按如下方式计算单隐藏层多层感知机的输出$O∈R^{n×q}$：
$$
H=XW^{(1)} +b^{(1)}\\
O=HW^{(2)}+b^{(2)}
$$
$H ∈ R^{n×h}$表示隐藏层的输出,$W^{(1)} ∈R^{d×h}$表示隐藏层权重，$b^{(1)} ∈R^{1×h}$表示隐藏层偏置以及输出层权重$W^{(2)} ∈R^{d×h}$ 和输出层偏置$b^{(2)} ∈R^{1×h}$

对于任意权重值，我们只需合并隐藏层，便可产生具有参数$W = W^{(1)}W^{(2)}$和$b=b^{(1)}W^{(2)} +b^{(2)}$ 的等价单层模型：
$$
O=(XW^{(1)} +b^{(1)})W^{(2)} +b^{(2)} = XW^{(1)}W^{(2)} +b^{(1)}W^{(2)} +b^{(2)} = XW+b.
$$
为了让多层感知机和线性模型进行区分，我们还需要一个激活函数

$$
H=σ(XW^{(1)} +b^{(1)})\\
O=HW^{(2)}+b^{(2)}
$$
为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，例如$H^{(1)}=σ(XW^{(1)} +b^{(1)})$和$H^{(2)}=
σ2(H^{(1)}W^{(2)} +b^{(2)})$，一层叠一层，从而产生更有表达能力的模型。

### 激活函数

激活函数通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。

#### ReLU函数

`ReLU`提供了一种非常简单的非线性变换。给定元素x，`ReLU`函数被定义为该元素与0的最大值：
$$
ReLU(x) = max(x,0).
$$
![1716702105477](D:\software\Typora\typora-user-images\1716702105477.png)

通俗地说，`ReLU`函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。使用`ReLU`的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且`ReLU`减轻了困扰以往神经网络的梯度消失问题

#### sigmoid函数

对于一个定义域在R中的输入，sigmoid函数将输入变换为区间(0,1)上的输出。
$$
sigmoid(x) =\frac{1}{1 +exp(−x)}
$$
![1716702139071](D:\software\Typora\typora-user-images\1716702139071.png)

sigmoid在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的`ReLU`所取代。在循环神经网络中，我们将描述利用sigmoid单元来控制时序信息流的架构。

#### tanh函数

与sigmoid函数类似，`tanh`(双曲正切)函数也能将其输入压缩转换到区间(‐1,1)上

$$
tanh(x) =\frac{1−exp(−2x)}{1+exp(−2x)}
$$
![1716702158996](D:\software\Typora\typora-user-images\1716702158996.png)

注意，当输入在0附近时，`tanh`函数接近线性变换。函数的形状类似于sigmoid函数，不同的是`tanh`函数关于坐标系原点中心对称。

多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。

### 模型选择、欠拟合和过拟合

将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合(over fitting)，用于对抗过拟合的技术
称为正则化(regularization)

**训练误差**是指，模型在**训练数据集**上计算得到的误差。

**泛化误差**是指，模型应用在同样从原始样本的分布中抽取的无限多数据样本时，**模型误差的期望**

K折交叉验证
当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是采用K折交叉验证。这里，原始训练数据被分成K个不重叠的子集。然后执行K次模型训练和验证，每次在K−1个子集上进行训练，并在剩余的一个子集(在该轮中没有用于训练的子集)上进行验证。最后，通过对K次实验的结果取平均来估计训练和验证误差。

![1715747643789](D:\software\Typora\typora-user-images\1715747643789.png)

如果没有足够的数据，简单的模型可能更有用。对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型。从一定程度上来说，深度学习目前的生机要归功于廉价存储、互联设备以及数字化经济带来的海量数据集。

### 权重衰减

在训练参数化机器学习模型时，权重衰减(weight decay)是最广泛使用的正则化的技术之一，它通常也被称为`L2`正则化。

主要是通过函数与零的距离来衡量函数的复杂度，因为在所有函数f中，函数f=0(所有输入都得到值0)在某种意义上是最简单的，一种简单的方法是通过线性函数f(x)=w⊤x中的权重向量的某个范数来度量其复杂性，例如$∥w∥^2$,可以让他小于一个固定值，比如$∥w∥^2<\theta$,我们通过正则化常数λ来描述这种权衡，这是一个非负超参数，我们使用验证数据拟合：
$$
L(w,b) + \frac{λ}{2}∥w∥^2,
$$
如果λ<0的话，后面的一项为0，如果λ>0的话可以限制$∥w∥^2$的大小，当我们取一个二次函数的导数时，2和1/2会抵消

L2正则化回归的小批量随机梯度下降更新如下式

$$
w←(1−ηλ)w− \frac{η}{|B|}\sum_{i∈B} x(i)( w^⊤x^{(i)} +b−y^{(i)}) .
$$
根据估计值与观测值之间的差异来更新w,然而，我们同时也在试图将w的大小缩小到零。这就是为什么这种方法有时被称为**权重衰减。**我们仅考虑惩罚项，优化算法在训练的每一步衰减权重。与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。较小的λ值对应较少约束的w，而较大的λ值对w的约束更大

### 暂退法(Dropout)

一个“好”的预测模型，能在未知的数据上有很好的表现；

平滑性:函数不应该对其输入的微小变化敏感。例如，当我们对图像进行分类时，我们预计向像素添加一些随机噪声应该是基本无影响的。

在训练过程中，在计算后续层之前向网络的每一层注入噪声。当训练一个有多层的深层网络时，注入噪声只会在输入‐输出映射上增强平滑性。这个想法被称为暂退法(dropout)。

将高斯噪声添加到线性模型的输入中。在每次训练迭代中，从均值为零的分布$ϵ∼N(0,σ^2)$ 采样噪声添加到输入x，从而产生扰动点$x′=x+ϵ$，预期是$E[x′]=x。$具体做法如下图所示

![1716101255448](D:\software\Typora\typora-user-images\1716101255448.png)

此处h和h'应当有一样的效果。h'是在对h进行dropout之后的结果

![1716101364287](D:\software\Typora\typora-user-images\1716101364287.png)

通常，我们在测试时不用暂退法。给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。然而也有一些例外：一些研究人员在测试时使用暂退法，用于估计神经网络预测的“不确定性”：如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。

### 前向、反向传播和计算图

前向传播：按顺序(从输入层到输出层)计算和存储神经网络中每层的结果。

反向传播：计算神经网络参数梯度的方法。简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。

在训练神经网络时，在初始化模型参数后，我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。

注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。

### 数值稳定性和模型初始化

- 梯度爆炸:参数更新过大，破坏了模型的稳定收敛；
- 梯度消失:参数更新过小,在每次更新时几乎不会移动,导致模型无法学习

#### 让训练更稳定

- 乘法变加法

  ResNet，LSTM

- 归一化

  梯度归一化，梯度剪裁

- 合理的权重初始和激活函数

### GPU

```cmd
nvidia-smi # 查看显卡信息
# 指定设备类型
torch.device('cpu')
torch.device('cuda')
# torch.device(f'cuda:{i}')来表示第i块GPU(i从0开始)
torch.device('cuda:1')
# 查询可用gpu的数量
torch.cuda.device_count()
```

## 卷积神经网络

卷积神经网络(convolutional neural network，CNN)是一类强大的、为处理图像数据而设计的神经网络。

对于高维感知数据，这种缺少结构的网络(MLP)可能会变得不实用。

假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。即使将隐藏层维度降低到1000，这个全连接层也将有106×103 =109个参数。想要训练这个模型将不可实现，因为需要有大量的GPU、分布式优化训练的经验和超乎常人的耐心。

### 不变性

#### 平移不变性

平移不变性(translation invariance):不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
$$
[H]_{i,j} = u +
\sum_{a}\sum_{b}[V]_{a,b}[X]_{i+a,j+b}
$$
我们是在使用系数`[V]a,b`对位置(i,j)附近的像素(i+a,j+b)进行加权得到`[H]i,j`。
注意:[V]a,b`的系数比`[V]i,j,a,b`少很多，因为前者不再依赖于图像中的位置。

#### 局部性

局部性(locality):神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。

如上所述，为了收集用来训练参数`[H]i,j`的相关信息，我们不应偏离到距(i,j)很远的地方。这意味着在|a| > ∆或|b| > ∆的范围之外，我们可以设置`[V]a,b = 0`。因此，我们可以将`[H]i,j`重写为
$$
[H]_{i,j} = u +
\sum_{a=-∆}^{∆}\sum_{b=-∆}^{∆}[V]_{a,b}[X]_{i+a,j+b}
$$
如上所述是一个卷积层，而卷积神经网络是包含卷积层的一类特殊的神经网络。

V被称为卷积核或者滤波器，亦或简单地称之为该卷积层的权重，通常该权重是可学习的参数。

#### 通道

图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量，比如包含1024×1024×3个像素。前两个轴与像素的空间位置有关，而第三个轴可以看作每个像素的多维表示。

因此，我们将X索引为`[X]i,j,k`。由此卷积相应地调整为`[V]a,b,c`，而不是`[V]a,b`。

为了支持输入X和隐藏表示H中的多个通道，我们可以在V中添加第四个坐标，即[V]a,b,c,d。
$$
[H]_{i,j,d} = 
\sum_{a=-∆}^{∆}\sum_{b=-∆}^{∆}\sum_{c}[V]_{a,b,c,d}[X]_{i+a,j+b,c}
$$
其中隐藏表示H中的索引d表示输出通道，而随后的输出将继续以三维张量H作为输入进入下一个卷积层。所以，可以定义具有多个通道的卷积层，而其中V是该卷积层的权重。

### 图像卷积

#### 互相关运算

在卷积层中，输入张量和核张量通过互相关运算产生输出张量。

![1716110878305](D:\software\Typora\typora-user-images\1716110878305.png)

蓝色部分是第一个输出元素:0 ×0+1×1+3×2+4×3=19，其他位置同理，在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。

#### 卷积层

卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。

```python
class Conv2D(nn.Module):    
    def __init__(self, kernel_size):        			    		super().__init__()        
        self.weight = nn.Parameter(torch.rand(kernel_size))         self.bias = nn.Parameter(torch.zeros(1))    
     def forward(self, x):        
        return corr2d(x, self.weight) + self.bias
```

高度和宽度分别为h和w的卷积核可以被称为h×w卷积或h×w卷积核。我们也将带有h×w卷积核的卷积层称为h×w卷积层。

#### 特征映射和感受野

上图中输出的卷积层有时被称为特征映射(featuremap)，因为它可以被视为一个输入映射到下一层的空间维度的转换器。

在卷积神经网络中，对于某一层的任意元素x，其感受野(receptivefield)是指在前向传播期间可能影响x计算的所有元素(来自所有先前层)

### 填充和步幅

假设输入形状为$n_h×n_w$，卷积核形状为$k_h×k_w$，那么输出形状将是$(n_h−k_h+1)×(n_w−k_w+1)$。因此，卷积的输出形状取决于输入形状和卷积核的形状。

#### 填充

在应用多层卷积时，我们常常丢失边缘像素。由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。但随着我们应用许多连续卷积层，累积丢失的像素数就多了。解决这个问题的简单方法即为填充

填充(padding)：在输入图像的边界填充元素(通常填充元素是0)

例如:上图中我们将3×3输入填充到5×5，那么它的输出就增加为4×4。阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素：0×0+0×1+0×2+0×3=0。

![1716112812830](D:\software\Typora\typora-user-images\1716112812830.png)

通常，如果我们添加$p_h$行填充(大约一半在顶部，一半在底部)和$p_w$列填充(左侧大约一半，右侧一半)，则输出形状将为
$$
(n_h −k_h +p_h +1)×(n_w −k_w +p_w +1)
$$


#### 步幅

## 循环神经网络

如果说卷积神经网络可以有效地处理空间信息，那么循环神经网络(recurrent neural network  RNN)则可以更好地处理序列信息。循环神经网络通过引入**状态变量**存储过去的信息和当前的输入，从而可以确定当前的输出。

### 序列模型

#### 自回归模型

假设在现实情况下相当长的序列$x_{t−1},...,x_1$可能是不必要的，因此我们只需要满足某个长度为τ的时间跨度，即使用观测序列$x_{t−1},...,x_{t-τ}$。当下获得的最直接的好处就是参数的数量总是不变的，至少在t >τ时如此，这种模型被称为自回归模型(autoregressive models)，因为它们是对自己执行回归。

如图所示，是保留一些对过去观测的总结$h_t$，并且同时更新预测$\hat{x}_t$和总结ht。这就产生了基于$\hat{x}_t = P(x_t | ht)$估计$x_t$，以及公式$h_t =g(h_{t−1},x_{t−1})$更新的模型。由于$h_t$从未被观测到，这类模型也被称为隐变量自回归模型(latent auto regressive models)。

![1716620345156](D:\software\Typora\typora-user-images\1716620345156.png)

总结:

- 序列模型的估计需要专门的统计工具，两种较流行的选择是自回归模型和隐变量自回归模型。
- 对于时间是向前推进的因果模型，正向估计通常比反向估计更容易。
- 对于直到时间步t的观测序列，其在时间步t+k的预测输出是“k步预测”。随着我们对预测时间k值的
  增加，会造成误差的快速累积和预测质量的极速下降。

### 文本预处理

文本的常见预处理步骤:

1. 将文本作为字符串加载到内存中。
2. 将字符串拆分为词元(如单词和字符)。
3. 建立一个词表，将拆分的词元映射到数字索引。
4. 将文本转换为数字索引序列，方便模型操作。

#### 词元化

词元(token)是文本的基本单位。

```python
def tokenize(lines, token='word'): #@save    
    """将文本行拆分为单词或字符词元"""    
    if token == 'word':        
        return [line.split() for line in lines]    
    elif token == 'char':        
        return [list(line) for line in lines]    
    else:        
        print('错误：未知词元类型：' + token)
```

#### 词表

词表(vocabulary)，用来将字符串类型的词元映射到从0开始的数字索引中。我们先将训练
集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为语料(corpus)

语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。

我们可以选择增加一个列表，用于保存那些被保留的词元，例如：填充词元(“<pad>”)；

序列开始词元(“<bos>”)；序列结束词元(“<eos>”)。

### 语言模型和数据集

#### 拉普拉斯平滑

假设长度为T的文本序列中的词元依次为$x_1,x_2,...,x_T$。于是，$x_t(1≤t≤T)$可以被认为是文
本序列在时间步t处的观测或标签。在给定这样的文本序列时，语言模型(language model)的目标是估计序
列的联合概率
$$
P(x_1,x_2,...,x_T).
$$
例如，只需要一次抽取一个词元$x_t∼P(x_t |x_{t−1},...,x_1)$，一个理想的语言模型就能够基于模型本身生成自
然文本。

为了训练语言模型，我们需要计算单词的概率，以及给定前面几个单词后出现某个单词的条件概率。这些概
率本质上就是语言模型的参数。

$$P(deep,learning,is,fun) = P(deep)P(learning | deep)P(is | deep,learning)P(fun | deep,learning,is).$$

训练数据集中词的概率可以根据给定词的相对词频来计算,例如可以通过计算某个单词在数据集中出现的次数除以总单词数可以预测该词出现的概率

$$
\hat{P}(learning | deep) =\frac{n(deep, learning)}{n(deep)} 
$$
其中n(x)和n(x,x′)分别是单个单词和连续单词对的出现次数。有一个问题是n(x,x′)在数据集中出现的次数远小于n(x)，比如连续单词对“deep learning”的出现频率要低得多，所以估计这类单词正确的概率要困难得多。如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。

解决上述问题的常见的策略是执行某种形式的拉普拉斯平滑(Laplace smoothing)，

$$
\hat{P}(x) = \frac{n(x)+ϵ1/m}{n+ϵ1} \\
\hat{P’}(x'|x) = \frac{n(x,x')+ϵ2\hat{P}(x')}{n(x)+ϵ2}\\
\hat{P''}(x''|x,x') = \frac{n(x,x',x'')+ϵ3\hat{P}(x'')}{n(x,x')+ϵ3}
$$
n表示训练集中的单词总数，用m表示唯一单词的数量,，ϵ1,ϵ2和ϵ3是超参数。以ϵ1为例：当ϵ1 =0时，不应用平滑；当ϵ1接近正无穷大时，ˆ P(x)接近均匀概率分布1/m。

然而，这样的模型很容易变得无效，原因如下：首先，我们需要存储所有的计数；其次，这完全忽略了单词
的意思。例如，“猫”(cat)和“猫科动物”(feline)可能出现在相关的上下文中，但是想根据上下文调整这
类模型其实是相当困难的。最后，长单词序列大部分是没出现过的，因此一个模型如果只是简单地统计先前
“看到”的单词序列频率，那么模型面对这种问题肯定是表现不佳的。

#### 自然语言统计

$$
P(x1,x2,x3,x4) = P(x1)P(x2)P(x3)P(x4)\\
  P(x1,x2,x3,x4) = P(x1)P(x2 | x1)P(x3 | x2)P(x4 | x3)\\
P(x1,x2,x3,x4) = P(x1)P(x2 | x1)P(x3 | x1,x2)P(x4 | x2,x3).
$$

涉及一个、两个和三个变量的概率公式分别被称为一元语法(unigram)、二元语法(bigram)和三
元语法(trigram)模型。

最流行的词看起来很无聊，这些词通常被称为停用词(stop words)，因此可以被过滤掉。尽管如此，它们本身仍然是有意义的，

#### 读取长序列数据

当序列变得太长而不能被模型一次性全部处理时，我们希望拆分这样的序列方便模型读取,任意长的序列可以
被我们划分为具有相同时间步数的子序列,当训练我们的神经网络时,这样的小批量子序列将被输入到模型中。

![1716626563235](D:\software\Typora\typora-user-images\1716626563235.png)

分割文本时，**不同的偏移量会导致不同的子序列**

##### 随机采样

在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。在迭代过程中，来自两个相邻的、随机
的、小批量中的子序列不一定在原始序列上相邻。

##### 顺序分区

在迭代过程中，除了对原始序列可以随机抽样外，我们还可以保证两个相邻的小批量中的子序列在原始序列
上也是相邻的。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。

 ### 循环神经网络

隐变量模型：
$$
P(x_t | x_{t−1},...,x_1) ≈ P(x_t | h_{t−1}),
$$
$h_{t−1}$是隐状态(hidden state)，也称为隐藏变量(hidden variable)，它存储了到时间步t−1的序列信
息。通常，我们可以基于当前输入$x_t$和先前隐状态$h_{t−1}$来计算时间步t处的任何时间的隐状态：
$$
h_t = f(x_t,h_{t−1})
$$
循环神经网络(recurrent neural networks，RNNs)是具有隐状态的神经网络。

下图展示了循环神经网络在三个相邻时间步的计算逻辑。在任意时间步t，隐状态的计算可以被视为：

1. 拼接当前时间步t的输入$X_t$和前一时间步t−1的隐状态$H_{t−1}$；
2. 将拼接的结果送入带有激活函数ϕ的全连接层。全连接层的输出是当前时间步t的隐状态Ht。

![1716627390424](D:\software\Typora\typora-user-images\1716627390424.png)

模型参数是$W_{xh}$和$W_{hh}$的拼接，以及$bh$的偏置,当前时间步t的隐状态$Ht$ 将参与计算下一时间步t+1的隐状态$H_{t+1}$。而且Ht还将送入全连接输出层，用于计算当前时间步t的输出$O_t$

#### 基于循环神经网络的字符级语言模型

使用字符级语言模型(character‐level language model)，将文本词元化为字符而不是单词

![1716627671731](D:\software\Typora\typora-user-images\1716627671731.png)

输入序列和标签序列分别为“machin”和“achine”,在训练过程中，我们对每个时间步的输出层的输出进行`softmax`操作，然后利用交叉熵损失计算模型输出和标签之间的误差。

#### 困惑度(Perplexity)

如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。一个更好的语言模型应该能让我们更准确地预测下一个词元。因此，它应该允许我们在压缩序列时花费更少的比特。所以我们可以通过一个序列中所有的n个词元的交叉熵损失的平均值来衡量：
$$
\frac{1}{n}\sum_{t=1}^n−logP(x_t | x_{t−1},...,x_1),
$$
其中P由语言模型给出，xt是在时间步t从该序列中观察到的实际词元。这使得不同长度的文档的性能具有了
可比性。由于历史原因，自然语言处理的科学家更喜欢使用一个叫做困惑度(perplexity)的量。简而言之，
它是上述公式的指数：
$$
exp(\frac{1}{n}\sum_{t=1}^n−logP(x_t | x_{t−1},...,x_1))
$$
困惑度的最好的理解是“下一个词元的实际选择数的调和平均数”

- 在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。
- 在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。
- 在基线上，该模型的预测是词表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词表中唯一
  词元的数量。

#### 梯度裁剪

## 现代循环神经网络

### 门控循环单元(GRU)

门控循环单元与普通的循环神经网络之间的关键区别在于:

GRU支持隐状态的门控。这意味着模型有专门的机制来确定应该何时更新隐状态，以及应该何时重置隐状态。

#### 重置门和更新门

重置门:决定是否忽略长短期依赖信息，即决定是否忽略之前的状态，只关注当前的输入。

更新门:决定有多少之前的记忆要保留，多少新信息要加入。更新门帮助网络决定在当前时间步应该更新多少之前的隐藏状态。

![1716887915302](D:\software\Typora\typora-user-images\1716887915302.png)

门控循环单元的数学表达
$$
R_t =σ(X_tW_{xr} +H_{t−1}W_{hr} +b_r)\\
Z_t = σ(X_tW_{xz} +H_{t−1}W_{hz} +b_z)
$$
$H_{t−1}∈R_{n× h}$是上一个时间步的隐状态,$Wxr$,$Wxz ∈ Rd× h$ 和$Whr$,$Whz ∈ Rh× h$是权重参数，$br$,$bz ∈ R1× h$是偏置参数

我们使用sigmoid函数将输入值转换到区间(0,1)。

#### 候选隐状态

将重置门与常规隐状态更新机制集成，得到在时间步t的候选隐状态(candidate
hidden state)$\tilde{H}_t ∈ R_{n× h}$
$$
\tilde{H}_t =tanh(X_tW_{xh} +(R_t ⊙H_{t−1})W_{hh} +b_h)
$$
符号⊙是Hadamard积(按元素乘积)运算符。在这里，我们使用tanh非线性激活函数来确保候选隐状态中的值保持在区间(−1,1)中。

![1716888695331](D:\software\Typora\typora-user-images\1716888695331.png)

观察上述公式可以发现，$R_t$和$H_{t−1} $的元素相乘可以减少以往状态的影响。每当重置门$R_t$中的项接近1时，上式就恢复成一个普通的循环神经网络。当重置门$R_t$中有的项接近0时，候选隐状态是以$X_t$作为输入的多层感知机的结果。因此，任何预先存在的隐状态都会被重置为默认值。

#### 隐状态

上述的计算结果只是候选隐状态，我们仍然需要结合更新门$Z_t$的效果。这一步确定新的隐状态$H_t∈R_{n× h}$在多大程度上来自旧的状态$H_{t−1}$和新的候选状态$\tilde{H}_t$。更新门Zt仅需要在$H_{t−1}$和$\tilde{H}_t$之间进行按元素的凸组合就可以实现这个目标。这就得出了门控循环单元的最终更新公式：
$$
H_t =Z_t ⊙H_{t−1} +(1−Z_t)⊙ \tilde{H}_t
$$
当更新门Zt接近1时，模型就倾向只保留旧状态,相反，当Zt接近0时，新的隐状态Ht就会接近候选隐状态$\tilde{H}_t$,这些设计可以帮助我们处理循环神经网络中的梯度消失问题，并更好地捕获时间步距离很长的序列的依赖关系。

![1716889459862](D:\software\Typora\typora-user-images\1716889459862.png)

总之，门控循环单元具有以下两个显著特征：

- 重置门有助于捕获序列中的短期依赖关系；
- 更新门有助于捕获序列中的长期依赖关系。

### 长短期记忆网络(LSTM)

#### 门控记忆元

记忆元是隐状态的一种特殊类型，它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。为了控制记忆元，我们需要许多门。

- 其中一个门用来从单元中输出条目，我们将其称为输出门(output gate)。
- 另外一个门用来决定何时将数据读入单元，我们将其称为输入门(input gate)。
- 我们还需要一种机制来重置单元的内容，由遗忘门(forget gate)来管理

和之前的GRU一样，当前时间步的输入和前一个时间步的隐状态作为数据送入长短期记忆网络的门中，它们由三个具有sigmoid激活函数的全连接层处理，以计算输入门、遗忘门和输出门的值。因此，这三个门的值都在(0,1)的范围内。

![1716890486222](D:\software\Typora\typora-user-images\1716890486222.png)

三个门的数学表达如下:
$$
I_t = σ(X_tW_{xi} +H_{t−1}W_{hi} +b_i)\\
F_t = σ(X_tW_{xf} +H_{t−1}W_{hf} +b_f)\\
O_t =σ(X_tW_{xo} +H_{t−1}W_{ho} +b_o)
$$

#### 候选记忆元

候选记忆元(candidate memory cell)$˜ Ct ∈Rn× h$计算与上面描述的三个门的计算类似，但是使用tanh函数作为激活函数，函数的值范围为(−1,1)。

$$
\tilde{C}_t = tanh(X_tW_{xc} +H_{t−1}W_{hc} +b_c)
$$
![1716902613954](D:\software\Typora\typora-user-images\1716902613954.png)

#### 记忆元

在长短期记忆网络中，输入门和遗忘门用来控制输入和遗忘(或跳过):输入门It控制采用多少来自$\tilde{C}_ t$的新数据，而遗忘门$F_t$控制保留多少过去的记忆元$C_{t−1}∈R_{n× h}$的内容。
$$
C_t = F_t ⊙C_{t−1} +I_t ⊙ \tilde{C}_ t.
$$
如果遗忘门始终为1且输入门始终为0，则过去的记忆元Ct−1将随时间被保存并传递到当前时间步。引入这种
设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。

![1716902949466](D:\software\Typora\typora-user-images\1716902949466.png)

#### 隐状态

最后，我们需要定义如何计算隐状态$Ht∈Rn× h$,在长短期记忆网络中，它仅仅是记忆元的tanh的门控版本。这就确保了Ht的值始终在区间(−1,1)内：
$$
H_t =O_t⊙tanh(C_t).
$$
只要输出门接近1，我们就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近0，我们只保留记
忆元内的所有信息，而不需要更新隐状态。

总结:

长短期记忆网络是典型的具有重要状态控制的**隐变量自回归模型**。多年来已经提出了其许多变体，例如，多
层、残差连接、不同类型的正则化。然而，由于序列的长距离依赖性，训练长短期记忆网络和其他序列模型
(例如门控循环单元)的成本是相当高的。

### 深度循环神经网络

![1716903824370](D:\software\Typora\typora-user-images\1716903824370.png)

我们可以将深度架构中的函数依赖关系形式化，这个架构是由上图中描述了L个隐藏层构成。

第l层的隐状态可描述为:

$$
H^{(l)}_t=ϕ(H^{(l−1)}_tW^{(l)}_{xh} +H^{(l−1)}_tW^{(l)}_{hh} +b^{(l)}_h )
$$
$W^{(l)}_{xh}$,$W^{(l)}_{hh}$,$b^{(l)}_h$都是第l个隐藏层的模型参数。

最后，输出层的计算仅基于第l个隐藏层最终的隐状态：
$$
O_t =H^{(L)}_tW_{hq} +b_q,
$$
里面的参数都是输出层的模型参数

### 双向循环神经网络

![1716986597836](D:\software\Typora\typora-user-images\1716986597836.png)

双向循环网络不只是在前向模式下“从第一个词元开始运行”的循环神经网络，也是从最后一个词元开始从后向
前运行”的循环神经网络。双向循环神经网络(bidirectional RNNs)添加了反向传递信息的隐藏层，以便更灵活地处理此类信息，上图是一个具有一个隐藏层的双向循环神经网络的架构

对于任意时间步t，给定一个小批量的输入数据$X_t∈R_{n× d}$(样本数n，每个示例中的输入数d)，并且令隐藏
层激活函数为ϕ。在双向架构中，我们设该时间步的前向和反向隐状态分别为$\overrightarrow{H}_t∈R_{n× h}$和$\overleftarrow{H}_t∈R_{n× h}$，其中h是隐藏单元的数目。前向和反向隐状态的更新如下：
$$
\overrightarrow{H}_t =ϕ(X_tW^{(f)}_{xh} +\overrightarrow{H}_{t−1}W^{(f)}_{hh} +b^{(f)}_h)\\
\overleftarrow{H}t =ϕ(X_tW^{(f)}_{xh} +\overleftarrow{H}_{t+1}W^{(f)}_{hh} +b^{(f)}_h)
$$
接下来，将前向隐状态−→ Ht和反向隐状态←− Ht连接起来，获得需要送入输出层的隐状态Ht∈Rn× 2h。该信息作为输入传递到下一个双向层。最后，输出层计算得到的输出为Ot ∈Rn× q(q是输出单元的数目)：
$$
O_t =H_tW_{hq} +b_q
$$
双向循环神经网络的缺陷:

- 模型在预测下一个词元时，无法知道下一个词元的下文是什么，所以将不会得到很好的精度。具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词；而在测试期间，我们只有过去的数据，因此精度将会很差。
- 另一个严重问题是，双向循环神经网络的计算速度非常慢。其主要原因是网络的前向传播需要在双向层中进行前向和后向递归，并且网络的反向传播还依赖于前向传播的结果。因此，梯度求解将有一个非常长的链。

### 机器翻译与数据集

机器翻译(machine translation)指的是将序列从一种语言自动翻译成另一种语言。

一般步骤:

#### 下载和预处理数据集

#### 词元化

在机器翻译中，我们更喜欢单词级词元化(最先进的模型可能使用更高级的词元化技术)

单词级词元化(Word-level tokenization)和字符级词元化(Character-level tokenization)是自然语言处理(NLP)中文本处理的两种不同方法，它们在处理文本数据时有不同的侧重点和应用场景：

1. **单词级词元化(Word-level tokenization)**：
   - 这是最常用的文本处理方法之一，涉及将文本分割成单词或词汇单元。
   - 单词级词元化通常使用空格和标点符号作为分隔符，将文本拆分成可识别的单词。
   - 这种方法适用于词汇量大、词汇重复率高的语言，如英语，因为它可以利用预先定义好的词汇表(vocabulary)来处理文本。
   - 优点是处理速度快，模型参数数量相对较少，易于实现。
   - 缺点是对于词汇量较小或不规则的词汇(如专有名词、新词)处理能力有限，可能导致信息丢失。

2. **字符级词元化(Character-level tokenization)**：
   - 字符级词元化是将文本分割成单个字符或字符组合的过程。
   - 这种方法不依赖于预先定义的词汇表，而是直接从字符级别学习语言的模式。
   - 字符级词元化适用于词汇量较小或存在大量未知词汇的语言，以及那些字符重复率高的语言。
   - 优点是灵活性高，能够处理任意词汇，包括未知词汇和拼写错误。
   - 缺点是模型参数数量多，计算复杂度高，可能导致训练时间长。

在实际应用中，选择单词级还是字符级词元化取决于特定任务的需求和语言特性。例如，在处理英语文本时，单词级词元化通常是首选，因为英语的词汇量大，且存在大量的共享词汇。而在处理中文、日文等没有明显单词分隔的语言时，字符级词元化可能更为合适，因为这些语言的文本不以空格分隔单词。

另外，一些现代NLP模型，如BERT，采用了一种称为“Word Piece”的词元化方法，它结合了单词级和字符级词元化的优点，能够更好地处理未知词汇和不规则表达。

#### 词表

机器翻译数据集是由源语言和目标语言的语言对组成，可以分别为源语言和目标语言构建两个词表。使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。为了解决这个问题，根据词元出现频率做如下规定：

- 次数少于2次的低频率词元视为相同的未知(“<unk>”)词元
- 序列的开始词元(“<bos>”)和结束词元(“<eos>”)
- 在小批量时用于将序列填充到相同长度的填充词元(“<pad>”)

#### 加载数据集

在机器翻译中，每个样本都是由源和目标组成的文本序列对，其中的每个文本序列可能具有不同的长度。但是，语言模型中的序列样本都有一个固定的长度，为了提高计算效率，我们仍然可以通过截断(truncation)和填充(padding)方式实现一次只处理一个小批量的文本序列。

- 如果文本序列的词元数目少于num_steps时，我们将继续在其末尾添加特定的“<pad>”词元，直到其长度达到num_steps；
- 反之，我们将截断文本序列时，只取其前num_steps 个词元，并且丢弃剩余的词元。

#### 训练模型

### 编码器-解码器架构

机器翻译的输入和输出都是长度可变的序列。为了处理这种类型的输入和输出，我们可以设计一个包含两个主要组件的架构：

- 第一个组件是一个编码器(encoder)：它接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。
- 第二个组件是解码器(decoder)：它将固定形状的编码状态映射到长度可变的序列。

这被称为编码器-解码器(encoder‐decoder)架构

![1717051133230](D:\software\Typora\typora-user-images\1717051133230.png)

### 序列到序列学习(seq2seq)

循环神经网络编码器使用长度可变的序列作为输入，将其转换为固定形状的隐状态。换言之，输入序列的信息被编码到循环神经网络编码器的隐状态中。为了连续生成输出序列的词元，独立的循环神经网络解码器是基于输入序列的编码信息和输出序列已经看见的或者生成的词元来预测下一个词元。

![1717053117831](D:\software\Typora\typora-user-images\1717053117831.png)

上图中的“<eos>”表示序列结束词元。一旦输出序列生成此词元，模型就会停止预测。

“<bos>”表示序列开始词元，它是解码器的输入序列的第一个词元。

#### 编码器

编码器将长度可变的输入序列转换成形状固定的上下文变量c，并且将输入序列的信息在该上
下文变量中进行编码。可以使用循环神经网络来设计编码器。

在时间步t，循环神经网络将词元xt的输入特征向量xt和$h_{t−1}$(即上一时间步的隐状态)转换为ht(即
当前步的隐状态)。
$$
h_t = f(x_t,h_t−1)
$$
编码器通过选定的函数q，将所有时间步的隐状态转换为上下文变量：

$$
c =q(h_1,...,h_T).
$$
上下文变量仅仅是输入序列在最后时间步的隐状态$h_T$

我们可以使用双向循环神经网络构造编码器，其中隐状态依赖于两个输入子序列，两个子序列是由隐状态所在的时间步的位置之前的序列和之后的序列(包括隐状态所在的时间步)，因此隐状态对整个序列的信息都进行了编码

这里实现双向循环神经网络使用了嵌入层，它的主要作用是将离散的单词或标记(tokens)转换为连续的向量表示。嵌入层的权重是一个矩阵，其行数等于输入词表的大小(vocab_size)，其列数等于特征向量的维度(embed_size)。对于任意输入词元的索引i，嵌入层获取权重矩阵的第i行(从0开始)以返回其特征向量。

#### 解码器

编码器输出的上下文变量$\mathbf{c}$ 对整个输入序列$x_1, \ldots, x_T$进行编码，解码器输出$y_{t'}$的概率取决于先前的输出子序列 𝑦1,…,𝑦𝑡′−1和上下文变量$\mathbf{c}$， 即$P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})$。

为了对上述解码器描述进行建模，我们使用另一种循环神经网络进行建模。循环神经网络将来自上一时间步的输出$y_{t^\prime-1}$ 和上下文变量$\mathbf{c}$作为其输入， 然后在当前时间步将它们和上一隐状态 $\mathbf{s}_{t^\prime-1}$转换为隐状态$\mathbf{s}_{t^\prime}$

$$
\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1})
$$
获得解码器的隐状态之后， 我们可以使用输出层和softmax操作 来计算在时间步$t^\prime$时输出$y_{t^\prime}$的条件概率分布 

$P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})$

![1717134819176](D:\software\Typora\typora-user-images\1717134819176.png)

#### 损失函数

在每个时间步，解码器预测了输出词元的概率分布。类似于语言模型，可以使用softmax来获得分布，并通过
计算交叉熵损失函数来进行优化。

#### 训练

训练的时候，特定的序列开始词元(“<bos>”)和原始的输出序列(不包括序列结束词元“<eos>”)拼接在一起作为解码器的输入。这被称为强制教学(teacher forcing)，因为原始的输出序列(词元的标签)被送入解码器。或者，将来自上一个时间步的预测得到的词元作为解码器的当前输入。

#### 预测

为了采用一个接着一个词元的方式预测输出序列，每个解码器当前时间步的输入都将来自于前一时间步的预
测词元。与训练类似，序列开始词元(“<bos>”)在初始时间步被输入到解码器中。该预测过程如图9.7.3所
示，当输出序列的预测遇到序列结束词元(“<eos>”)时，预测就结束了。

#### 预测序列的评估

`BLEU`值(Bilingual Evaluation Understudy)是自然语言处理(NLP)领域中用于评估机器翻译质量的一种度量标准。它通过计算机器翻译输出与一组参考翻译之间的重叠程度来工作。`BLEU`分数范围从0到1，其中1表示完美的匹配，0表示没有重叠。
$$
\exp\left(\min\left(0, 1 - \frac{\mathrm{len}*{\text{label}}}{\mathrm{len}*{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n}
$$
其中$len_{label}$表示标签序列中的词元数和$len_{pred}$表示预测序列中的词元数，k是用于匹配的最长的n元语法。另外，用$p_n$表示n元语法的精确度，它是两个数量的比值：第一个是预测序列与标签序列中匹配的n元语法的数
量，第二个是预测序列中n元语法的数量的比率。

给定标签序列$A$、$B$、$C$、$D$、$E$、$F$ 和预测序列$A$、$B$、$B$、$C$、$D$， 我们有$p_1 = 4/5$、$p_2 = 3/4$、$p_3 = 1/3$和$p_4 = 0$

`BLEU`分数是一种快速且广泛接受的评估机器翻译质量的方法，尽管它有一些局限性，比如它不能很好地评估语义的准确性和流畅性，但它仍然是研究和工业界中常用的评估标准之一。在实际使用中，通常会结合其他指标(如METEOR、ROUGE等)来更全面地评估翻译质量。

###  束搜索

之前我们预测的时候，我们是逐个预测输出序列，直到预测序列中出现特定的序列结束词元“<eos>”

#### 贪心搜索

对于输出序列的每一时间步$t'$， 我们都将基于贪心搜索从$\mathcal{Y}$中找到具有最高条件概率的词元，即：
$$
y_{t'} = \operatorname*{argmax}_{y \in \mathcal{Y}} P(y \mid y_1, \ldots, y*{t'-1}, \mathbf{c})
$$
一旦输出序列包含了“<eos>”或者达到其最大长度$T'$，则输出完成

![1717138046852](D:\software\Typora\typora-user-images\1717138046852.png)

如图，在预测输出序列“A”“B”“C”和“<eos>”。这个输出序列的条件概率是0.5×0.4×0.4×0.6=0.048。

贪心搜索的问题是贪心搜索无法保证得到最优序列。

![1717137954964](D:\software\Typora\typora-user-images\1717137954964.png)

在图9.8.2中的时间步4生成每个词元的条件概率也不同于图9.8.1中的条件概率。结果，图9.8.2中的输出序列“A”“C”“B”和“”的条件概率为0.5×0.3×0.6×0.6=0.054，这大于图9.8.1中的贪心搜索的条件概率。

这个例子说明：**贪心搜索获得的输出序列不一定最佳**。

#### 穷举搜索

穷举搜索(exhaustive search)：穷举地列举所有可能的输出序列及其条件概率，然后计算输出条件概率最高的一个。

#### 束搜索

如果精度最重要，则显然是穷举搜索。如果计算成本最重要，则显然是贪心搜索。而束搜索的实际应用则介于这两个极端之间。

*束搜索*(beam search)是贪心搜索的一个改进版本。 它有一个超参数，名为*束宽*(beam size)$k$。 在时间步$1$，我们选择具有最高条件概率的$k$个词元。 这$k$个词元将分别是$k$个候选输出序列的第一个词元。 在随后的每个时间步，基于上一时间步的$k$个候选输出序列， 我们将继续从$k\left|\mathcal{Y}\right|$个可能的选择中 挑出具有最高条件概率的$k$个候选输出序列。

![1717138254319](D:\software\Typora\typora-user-images\1717138254319.png)

束搜索计算
$$
\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\mid \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})
$$
其中$L$是最终候选序列的长度， 𝛼通常设置为$0.75$。 因为一个较长的序列在上述式子的求和中会有更多的对数项， 因此分母中的$L^\alpha$用于惩罚长序列。

束搜索的计算量为$\mathcal{O}(k\left|\mathcal{Y}\right|T')$， 这个结果介于贪心搜索和穷举搜索之间。 实际上，贪心搜索可以看作一种束宽为$1$的特殊类型的束搜索。 通过灵活地选择束宽，束搜索可以在正确率和计算代价之间进行权衡。

## 注意力机制

### Q&K&V

- 查询(Query): 指的是查询的范围，自主提示，即主观意识的特征向量

- 键(Key): 指的是被比对的项，非自主提示，即物体的突出特征信息向量

- 值(Value) :  则是代表物体本身的特征向量，通常和Key成对出现

  注意力机制是通过Query与Key的注意力汇聚(给定一个 Query，计算Query与 Key的相关性，然后根据Query与Key的相关性去找到最合适的 Value)实现对Value的注意力权重分配，生成最终的输出结果
  ![image-20240425202647313](D:\software\Typora\typora-user-images\image-20240425202647313.png)

### 注意力汇聚:

### Nadaraya-Watson 核回归

注意力机制的主要成分是:查询(自主提示)和键(非自主提示)之间的交互形成了**注意力汇聚**；注意力汇聚有选择地聚合了值(感官输入)以生成最终的输出。

#### 平均汇聚

基于平均汇聚来计算所有训练样本输出值的平均值：
$$
f(x) = \frac{1}{n}\sum_{i=1}^{n} \hat{y}_i
$$
![1717297609087](D:\software\Typora\typora-user-images\1717297609087.png)

根据平均汇聚得到的真实函数f（“Truth”）和预测函数（“Pred”）相差很大。

#### 非参数注意力汇聚

由于平均汇聚忽略了输入xi，所以提出了基于Nadaraya和Watson 的核回归，根据输入的位置对输出yi进行加权:
$$
f(x) = \sum_{i=1}^n\frac{K(x-x_i)}{\sum_{j=1}^{n}(K(x-x_j))}
$$
其中K是核（kernel）,上述公式也被称为Nadaraya-Watson核回归（Nadaraya‐Watson kernel
regression）

受此启发，我们可以从注意力机制框架的角度重写上述公式，成为一个更加通用的注意力汇聚(attention pooling)公式:
$$
f(x) =\sum_{i=1}^{n}\alpha(x,x_i)y_i
$$
其中x是查询，(xi , yi)是键值对。注意力汇聚是yi的加权平均**。将查询x和键xi之间的关系建模为注意力权重**,这个权重将被分配给每一个对应值yi。 **对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布:它们是非负的，并且总和为1。**

如果一个键xi越是接近给定的查询x，那么分配给这个键对应值yi的注意力权重就会越大，也就“获得了更多的注意力”。

为了更好地理解注意力汇聚，下面考虑一个高斯核（Gaussiankernel），其定义为：
$$
K(u) = \frac{1}{\sqrt{2\pi}} exp(\frac{−u^2}
2 ).
$$
将高斯核代入注意力汇聚公式可得到如下结果
$$
\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\ \\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i = \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}
$$
![image-20240425213125542](D:\software\Typora\typora-user-images\image-20240425213125542.png)

基于这个非参数的注意力汇聚模型来绘制预测结果。从绘制的结果会发现新的模型预测线是平滑的，并且比平均汇聚的预测更接近真实。

#### 带参数注意力汇聚

非参数的Nadaraya‐Watson核回归具有一致性（consistency）的优点:如果有足够的数据，此模型会收敛到 最优结果。尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中。

与非参数注意力汇聚略有不同，在下面的查询x和键xi之间的距离乘以可学习参数w:

![image-20240425213538751](D:\software\Typora\typora-user-images\image-20240425213538751.png)

##### 批量矩阵乘法

```python
X = torch.ones((2, 1, 4)) # 生成两个1*4的矩阵
Y = torch.ones((2, 4, 6)) # 生成2个4*6的矩阵
# 批量矩阵乘法
print(torch.bmm(X, Y).shape) # torch.Size([2, 1, 6])
print(torch.Size([2, 1, 6])) # torch.Size([2, 1, 6])
```

在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值

基于带参数的注意力汇聚，使用小批量矩阵乘法

![image-20240425215641641](D:\software\Typora\typora-user-images\image-20240425215641641.png)

可以看到随着迭代次数变多，损失是下降的，然后就是来观察真实值和预测值的拟合情况

![image-20240425215818538](D:\software\Typora\typora-user-images\image-20240425215818538.png)

虽然拟合度变高了，但是曲线不是很平滑，因为与非参数的注意力汇聚模型相比，带参数的模型加入可学习的参数后，曲线在注意力权重较大的区域变得更不平滑。

### 注意力评分函数

在注意力汇聚中，使用高斯核来对查询和键之间的关系建模。**高斯核指数部分可以视为注意力评分函数** ，简称评分函数，然后把这个函数的输出结果输入到softmax函数中进行运算。通过上述步骤，将得到与键对应的值的概率分布(**即注意力权重**)。最后，**注意力汇聚的输出就是基于这些注意力权重的值的加权和**。下图简单的描绘了这一过程，其中a表示注意力评分函数

![image-20240428135706873](D:\software\Typora\typora-user-images\image-20240428135706873.png)

如上图所示，选择不同的注意力评分函数a会导致不同的注意力汇聚操作，这里我们介绍两个流行的评分函数:

 #### 掩蔽softmax操作

如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该 被纳入到注意力汇聚中。例如有些文本序列含有无意义的词元，这种信息就不能作为有效值被处理，所以应该丢弃这种无意义的值。可以指定一个有效序列长度(即词元的个数),以便在计算softmax时过滤掉超出指定范围的位置

#### 加性注意力

一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。给定查询$q∈R_ q$和键$k∈ R_ k$,加性注意力(additive attention)的评分函数为
$$
a(q,k)=w_v^Ttanh(w_q^q+w_k^k)∈R
$$

#### 缩放点积注意力 

使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同的长度d。假设查询和键 的所有元素都是独立的随机变量，并且都满足零均值和单位方差，那么两个向量的点积的均值为0，方差为d。 为确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是1，我们再将点积除以$\sqrt{d}$，则缩放点积注意力评分函数为：
$$
a(q,k) = q^Tk/\sqrt{d}
$$
在实践中，我们通常从小批量的角度来考虑提高效率，例如基于n个查询和m个键－值对计算注意力，其中查询和键的长度为d，值的长度为v。查询$Q∈R_{n×d}$、键$K∈R_{m×d}$和值$V∈R_{m×v}$的缩放点积注意力是：
$$
softmax(Qk^T/\sqrt{d})V ∈ R^{n×v}
.
$$
小结 

1. 将注意力汇聚的输出计算可以作为值的加权平均，选择不同的注意力评分函数会带来不同的注意力汇 聚操作。 
2. 当查询和键是不同长度的矢量时，可以使用可加性注意力评分函数。当它们的长度相同时，使用缩放的 “点－积”注意力评分函数的计算效率更高。

### Bahdanau 注意力

在预测词元时，如果不是所有输入词元都相关，模型将仅对齐(或参与)输入序列中与当前预测相关的部分。这是通过将上下文变量视为**注意力集中的输出**来实现的。

#### 模型

Bahdanau注意力模型与seq2seq中的模型相同，只不过上下文变量c在任何解码时间步t′都会被$c_{t′}$替换。假设输入序列中有T个词元， 解码时间步t ′的上下文变量是注意力集中的输出：
$$
c_{t'}=\sum_{t=1}^{T}\alpha(s_{t'},h_t)h_t
$$
其中，时间步${t′−1}$时的解码器隐状态$s_{t ′−1}$是查询，编码器隐状态ht既是键也是值，注意力权重α是使用加性注意力打分函数计算的。

![image-20240429133756710](D:\software\Typora\typora-user-images\image-20240429133756710.png)

小结 

- 在预测词元时，如果不是所有输入词元都是相关的，那么具有Bahdanau注意力的循环神经网络编码 器‐解码器会有选择地统计输入序列的不同部分。这是通过将上下文变量视为加性注意力池化的输出来 实现的。 
- 在循环神经网络编码器‐解码器中，Bahdanau注意力将上一时间步的解码器隐状态视为查询，在所有时 间步的编码器隐状态同时视为键和值。

### 多头注意力

与其只使用单独一个注意力汇聚，我们可以用独立学习得到的h组不同的线性投影(linear projections) 来变换查询、键和值。然后，这h组变换后的查询、键和值将并行地送到注意力汇聚中。最后，将这h个注意 力汇聚的输出拼接在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。这种设计被称为**多头注意力**

![image-20240429144243556](D:\software\Typora\typora-user-images\image-20240429144243556.png)

#### 模型

给定查询q ∈ R dq、键k ∈ R dk和 值v ∈ R dv，每个注意力头hi(i = 1, . . . , h)的计算方法为：
$$
h_i=f(W_i^{(q)}q,W_i^{(k)}k,W_i^{v}v)∈ R^{p×v}
$$
代表注意力汇聚的函数f可以是加性注意力也可以是点积注意力

多头注意力的输出需要经过另一个线性转换，它对应着h个头连结后的结果，因此其可学习参数是$ W_o ∈ R ^{p_o×hp_v}$：
$$
Wo [h_1...h_h]^T ∈ R_{po}
$$
基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。

### 自注意力和位置编码

将词元序列输入注意力池化中，以便同一组词元同时充当查询、键和值。具体来说，每个 查询都会关注所有的键－值对并生成一个注意力输出。由于查询、键和值来自同一组输入，因此被称为 自注 意力(self‐attention),也被称为内部注意力(intra‐attention)

给定一个由词元组成的输入序列$x_1, . . . , x_n$，其中任意$x_i ∈ R_ d(1 ≤ i ≤ n)$。该序列的自注意力输出为一个长 度相同的序列$ y_1, . . . , y_n$，其中：
$$
y_i = f(x_i ,(x_1, x_1), . . . ,(x_n, x_n)) ∈ R^ d
$$

#### 位置编码

在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作。为了使用序列的顺序信息，通过在输入表示中添加 位置编码来注入绝对的或相对的位置信息。

### Transformer

Transformer作为编码器－解码器架构的一个实例,，Transformer的编码器和解码器是基于自注意力的模块叠加而成的，源（输入）序列和目标（输出）序列的嵌入（embedding）表示将加上位置编码（positional encoding），再分别输入到编码器和解码器中。

![1717507956637](D:\software\Typora\typora-user-images\1717507956637.png)

从宏观角度来看，Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为sublayer）。第一个子层是多头自注意力（multi‐head  self‐attention）汇聚；第二个子层是基于位置的前馈网络（position wise feed‐forward network）。具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。

Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中
描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为编码器－解码器注意力（encoder
decoder attention）层。在编码器－解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个
编码器的输出。在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。但是，解码器中的每个位
置只能考虑该位置之前的所有位置。这种掩蔽（masked）注意力保留了自回归（auto‐regressive）属性，确
保预测仅依赖于已生成的输出词元。

#### 基于位置的前馈网络

它的作用是对序列中每个位置的表示进行独立相同的处理，以增强模型的表达能力。对所有位置的表示进行变换时使用的是同一个多层感知机，他可以学习到输入表示的非线性变换，增强模型对复杂特征的捕捉能力，例如：输入X的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将会被一个两层的感知机转换成形状为（批量大小，时间步数，`ffn_num_outputs`）的输出张量。

#### 残差连接和层规范化

### 自然语言处理：预训练

![1717651834784](D:\software\Typora\typora-user-images\1717651834784.png)

预训练好的文本表示可以放入各种深度学习架构，应用于不同自然语言处理任务

#### 词嵌入（`Word2Vec`）

将单词映射到实向量的技术称为词嵌入虽然独热向量很容易构建，但它们通常不是一个好的选择。一个主要原因是独热向量不能准确表达不同词之间的相似度，比如我们经常使用的“余弦相似度”，因为任意两个不同词的独热向量之间的余弦相似度为0，所以独热向量不能编码词之间的相似性。

##### 自监督的`Word2Vec`

``Word2Vec``将每个词映射到一个固定长度的向量，这些向量能更好地表达不同词之间的相似性和类比关系``Word2Vec``工具包含两个模型，即跳元模型（skip‐gram）和连续词袋（`CBOW`）

##### 跳元模型（Skip-Gram）

以文本序列“the”“man”“loves”“his”“son”为例。跳元模型假设一个词可以用来在文本序列中生成其周围的单词。假设中心词选择“loves”，并将上下文窗口设置为2,，跳元模型考虑生成上下文词“the”“man”“him”“son”的条件概率：

![1717652324193](D:\software\Typora\typora-user-images\1717652324193.png)

给定中心词$w_c$（词典中的索引c），生成任何上下文词$w_o$（词典中的索引o）的条件概率可以通过对向量点积的`softmax`操作来建模：
$$
P(w_o | w_c) = \frac{exp(u^⊤_ov_c)}{\sum_{i∈V} exp(u^⊤_iv_c)}
$$

##### 连续词袋（CBOW）模型

连续词袋（CBOW）模型类似于跳元模型。与跳元模型的主要区别在于，连续词袋模型假设中心词是基于其
在文本序列中的周围上下文词生成的。例如以下的示例就是对连续词袋模型的解释
$$
P("loves" | "the","man","his","son")
$$
![1717763898777](D:\software\Typora\typora-user-images\1717763898777.png)

由于连续词袋模型中存在多个上下文词，因此在计算条件概率时对这些上下文词向量进行平均。
$$
P(w_c |(w_{o1}...w_{o2m}) ) = \frac{exp(\frac{1}{2m}u^⊤_ov_c)(v_{o1} + ...,+v_{o2m}
 )}{\sum_{i∈V} exp(\frac{1}{2m}u^⊤_iv_c)}
$$

#### 近似训练

可以看到，跳元模型和连续词袋模型的计算中都包含求和计算，不幸的是，在一个词典上（通常有几十万或数百万个单词）求和的梯度的计算成本是巨大的。为了降低上述计算复杂度，本节将介绍两种近似训练方法：负采样和分层softmax。

##### 负采样

负采样修改了原目标函数。给定中心词wc的上下文窗口，任意上下文词wo来自该上下文窗口的被认为是由下
式建模概率的事件：
$$
P(D =1|w_c,w_o) = σ(u^⊤_ov_c),  其中σ使用了sigmoid激活函数的定义：
$$
中间证明过程没看明白。。。

我们可以看到，现在每个训练步的梯度计算成本与词表大小无关，而是线性依赖于K。当将超参数K设置为
较小的值时，在负采样的每个训练步处的梯度的计算成本较小。

##### 层序Softmax

层序Softmax使用二叉树，其中树的每个叶节点表示词表V中的一个词。

![1717764885283](D:\software\Typora\typora-user-images\1717764885283.png)

由于二叉树结构，$L(w_o)−1$大约与$O(log2|V|)$是一个数量级。当词表大小V很大时，与没有近似训
练的相比，使用分层softmax的每个训练步的计算代价显著降低。

#### 全局向量的词嵌入（`GloVe`）

上下文窗口内的词共现可以携带丰富的语义信息。例如，在一个大型语料库中，“固体”比“气体”更有可能
与“冰”共现，可以预先计算此类共现的全局语料库统计数据：这可以提高训练效率

`GloVe`（Global Vectors for Word Representation）是一种用于生成词嵌入（word embeddings）的无监督学习算法，由斯坦福大学和Google的研究人员在2014年提出。`GloVe`模型通过捕获词语在语料库中的全局统计信息来学习词向量，这些向量能够反映词语之间的语义和语法关系。

以下是`GloVe`模型的一些关键特点：

1. **全局统计信息**：`GloVe`利用整个语料库中的词语共现统计信息来构建词向量。

2. **共现矩阵**：模型首先构建一个共现矩阵，记录任意两个词语在语料库中共同出现的次数。

3. **权重预测**：`GloVe`模型试图预测词语对的共现概率，通过最小化预测概率和实际共现频率之间的差异来训练词向量。

4. **对称性**：`GloVe`模型是对称的，即对于任意两个词语 \( $w_i $) 和 \($ w_j$ \)，模型学习到的向量对 \( ($u_i$, $u_j$) \) 和 \( ($u_j$, $u_i$) \) 是相同的，其中 \( u \) 表示词向量。

5. **词向量生成**：`GloVe`模型生成的词向量捕捉了词语之间的语义相似性，使得语义上相似的词语在向量空间中更接近。

6. **易于实现**：`GloVe`模型相对容易实现，且计算效率较高。

7. **适用于多种语言**：`GloVe`模型不依赖于特定的语言结构，因此可以应用于多种语言的词嵌入生成。

8. **与其他模型的比较**：与`Word2Vec`等其他词嵌入模型相比，`GloVe`在某些任务上能够提供更好的性能，尤其是在捕捉词语的语法信息方面。

9. **开源实现**：`GloVe`模型的源代码和预训练词向量是开源的，可以在网上找到，便于研究人员和开发者使用。

10. **应用广泛**：`GloVe`词向量被广泛应用于各种自然语言处理任务，如文本分类、情感分析、命名实体识别、机器翻译等。

`GloVe`模型因其生成高质量词嵌入的能力而受到重视，这些词嵌入可以显著提高许多NLP任务的性能。随着NLP领域的不断发展，`GloVe`和其他词嵌入技术将继续是研究和应用的重要基础。

##### 带全局语料统计的跳元模型



#### 子词嵌入

在英语中，“helps”“helped”和“helping”等单词都是同一个词“help”的变形形式。`Word2Vec`和`GloVe`都没有对词的内部结构进行探讨。

##### fastText模型

为了使用形态信息，fastText模型提出了一种子词嵌入方法，其中子词是一个字符n‐gram,fastText可以被认为是子词级跳元模型，而非学习词级向量表示，其中每个中心词由其子词级向量之和表示。

具体做法：

以单词“where”为例获得fastText中每个中心词的子词。

- 首先，在词的开头和末尾添加特殊字符“<”和“>”，以将前缀和后缀与其他子词区分开来。
- 然后，从词中提取字符n‐gram。例如，值n=3时，我们将获得长度为3的所有子词：“<wh”“whe”“her”“ere”“re>”和特殊子词“<where>”

##### 字节对编码（Byte Pair Encoding）

在fastText中，所有提取的子词都必须是指定的长度，例如3到6，因此词表大小不能预定义。为了在固定大小
的词表中允许可变长度的子词，我们可以应用一种称为字节对编码（Byte Pair Encoding，BPE）的压缩算法
来提取子词。

字节对编码（Byte Pair Encoding，简称BPE）是一种用于文本数据的子词编码方法，最初由OpenAI提出，并在GPT模型中使用。BPE的目标是将文本分解为更小的单元，这些单元可以是字节、字符或自定义的符号，以便于模型更好地处理词汇的变体和低频词汇

`BPE`算法实现的步骤

- 从单个字符开始，统计数据集中字符对的频率。
- 每次迭代中，选择频率最高的字符对并将其合并为一个新的子词单元。
- 重复合并过程，直到达到预定的词汇表大小或特定的迭代次数。

#### 词的相似性和类比任务

#### BERT

`word2vec`和`GloVe`都将相同的预训练向量分配给同一个词，而不考虑词的上下文,但是自然语言中的词有丰富的多义现象和复杂的语义，上下文无关表示具有明显的局限性。例如，在“a crane is flying”（一只鹤在飞）和“a crane driver came”（一名吊车司机来了），“crane”一词有完全不同的含义；

我们规定，词元x的上下文敏感表示是函数$f(x,c(x))$，其取决于x及其上下文$c(x)$。

流行的上下文敏感表示包括TagLM（language‐model‐augmented sequence tagger，语言模型增强的序列标记器）、CoVe（Context Vectors，上下文向量）和ELMo（Embeddings from Language Models，来自语言模型的嵌入）

例如，通过将整个序列作为输入，ELMo是为输入序列中的每个单词分配一个表示的函数。ELMo的表示将作为附加特征添加到下游任务的现有监督模型中，添加ELMo改进了六种自然语言处理任务的技术水平：情感分析、自然语言推断、语义角色标注、共指消解、命名实体识别和问答。

##### 从特定于任务到不可知任务

尽管ELMo显著改进了各种自然语言处理任务的解决方案，但每个解决方案仍然依赖于一个特定于任务的架
构。然而，为每一个自然语言处理任务设计一个特定的架构实际上并不是一件容易的事。GPT（Generative
Pre Training，生成式预训练）模型为上下文的敏感表示设计了通用的任务无关模型。与ELMo冻结预训练模型的参数不同，GPT在下游任务的监督学习过程中对预训练Transformer解码器中的所有参数进行微调。GPT在自然语言推断、问答、句子相似性和分类等12项任务上进行了评估，并在对模型架构进行最小更改的情况下改善了其中9项任务的最新水平。

然而，由于语言模型的自回归特性，GPT只能向前看（从左到右）。在“i went to the bank to deposit cash”（我去银行存现金）和“i went to the bank to sit down”（我去河岸边坐下）的上下文中，由于“bank”对其左边的上下文敏感，GPT将返回“bank”的相同表示，尽管它有不同的含义。

##### BERT：把两个最好的结合起来

如上所述，ELMo对上下文进行双向编码，但使用特定于任务的架构；而GPT是任务无关的，但是从左到右
编码上下文。BERT（来自Transformers的双向编码器表示）结合了这两个方面的优点。它对上下文进行双
向编码，并且对于大多数的自然语言处理任务只需要最少的架构改变。通过使用预训练的Transformer编码器，BERT能够基于其双向上下文表示任何词元。在下游任务的监督学习过程中，BERT在两个方面与GPT相似。

![1717936378247](D:\software\Typora\typora-user-images\1717936378247.png)

BERT表示将被输入到一个添加的输出层中，根据任务的性质对模型架构进行最小的更改，例如预测每个词元与预测整个序列。其次，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。

##### 输入表示

BERT输入序列明确地表示单个文本和文本对。

- 当输入为单个文本时，BERT输入序列是特殊类别词元“<cls>”、文本序列的标记、以及特殊分隔词元“<sep>”的连结。
- 当输入为文本对时，BERT输入序列是“<cls>、第一个文本序列的标记、“<sep>”、第二个文本序列标记、以及“<sep>”的连结。

![1717937424711](D:\software\Typora\typora-user-images\1717937424711.png)

# 机器翻译

## 统计语言建模基础

### KL 距离和熵

以球队夺冠概率来举例子，假设几只队伍的实力未知或者实力相当，那么人们就**很难对比赛结果做出预测。信息熵也相对较高**；如果这几只球队中某一支球队的实力可以碾压其他球队，那么人们对比赛结果的预测就会很明确。**结果是容易猜到的**，**信息量和信息熵也就相对较低**。因此可以得知：分布越尖锐熵越低，分布越均匀熵越高。

如果同一个随机变量X上有两个概率分布P(x)和Q(x)，那么可以使用KullbackLeibler 距离或 KL 距离（KL Distance）来衡量这两个分布的不同（也称作KL散度）。这种度量就是相对熵（Relative Entropy）它衡量的是同一个事件空间里两个概率分布的差异

交叉熵（Cross entropy）是一个与KL距离密切相关的概念，从实践的角度来说，交叉熵与KL距离的目的相同：都是用来描述两个分布的差异。由于交叉熵计算上更加直观方便，因此在机器翻译中被广泛应用。

### n-gram 语言模型

n-gram语言模型。它是一种经典的统计语言模型，而且在机器翻译及其他自然语言处理任务中有非常广泛的应用。其中n-gram表示n个连续单词构成的单元，也被称作n元语法单元。这个模型的数学描述如下：
$$
P(w_m|w_1w_2...w_{m−1}) =P(w_m|w_{m−n+1}...w_{m−1})
$$
![1718367568493](D:\software\Typora\typora-user-images\1718367568493.png)

#### 参数估计和平滑算法

对于n-gram 语言模型，每个$ P(w_m|w_{m−n+1}...w_{m−1}) $都可以被看作是模型的参数（Parameter）。而 n-gram 语言模型的一个核心任务是估计这些参数的值，即参数估计。通常，参数估计可以通过在数据上的统计得到。一种简单的方法是：给定一定数量的句子，统计每个n-gram出现的频次，并利用上述公式得到每个参数$ P(w_m|w_{m−n+1}...w_{m−1}) $的值。这个过程也被称作模型的训练（Training）

但是参数估计方法并不完美，因为它无法很好地处理低频或者未见现象。比如，如果语料中从没有“确实”和“现在”两个词连续出现的情况，即c(确实/现在)=0。
$$
P(现在|确实) =\frac{c(确实/现在)}{c(确实)} 
=\frac{0}{c(确实)} = 0
$$
显然，这个结果是不合理的。因为即使语料中没有“确实”和“现在”两个词连续出现，这种搭配也是客观存在的。

更常见的问题是那些根本没有出现在词表中的词，称为**未登录词**（Out of vocabulary Word，OOV Word），比如一些生僻词，可能模型训练阶段从来没有看到过，这时模型仍然会给出0概率。

![1718368029818](D:\software\Typora\typora-user-images\1718368029818.png)

为了解决未登录词引起的零概率问题，常用的做法是对模型进行**平滑（Smoothing）**，也就是给可能出现零概率的情况一个非零的概率，使得模型不会对整个序列给出零概率。

语言模型使用的平滑算法有很多。例如：**加法平滑法**、**古德图灵估计法**和**Kneser-Ney平滑**。这些方法也可以被应用到其他任务的概率平滑操作中。

**加法平滑（Additive Smoothing）**是一种简单的平滑技术。通常情况下，系统研发者会利用采集到的语料库来模拟真实的全部语料库。当然，没有一个语料库能覆盖所有的语言现象。加法平滑方法假设每个n-gram出现的次数比实际统计次数多θ次，0<θ≤1。这样，计算概率的时候分子部分不会为0。重新计算P(现在|确实)，可以得到：
$$
P(现在|确实) =\frac{θ+c(确实/现在)}{\sum^{|V|}_{w}(θ+c(确实/w))} 
=\frac{ θ+c(确实/现在)}{θ|V |+c(确实)}
$$
其中，V 表示词表，|V|为词表中单词的个数，w为词表中的一个词，c表示统计单词或短语出现的次数。有时候，加法平滑方法会将θ取1，这时称之为**加一平滑或是拉普拉斯平滑**。这种方法比较容易理解，也比较简单，因此常被用于对系统的快速实现上。

**古德-图灵估计（Good-Turing Estimate）**也是很多平滑算法的核心，其基本思路是：把非零的n元语法单元的概率降低，匀给一些低概率n元语法单元，以减小最大似然估计与真实概率之间的偏离

`Kneser-Ney `平滑方法是被广泛认为是最有效的平滑方法之一

#### 语言模型的评测

困惑度（Perplexity，PPL）是一种衡量语言模型的好坏的指标。对于一个真实的词序列$w_1...w_m$，困惑度被定义为：
$$
PPL = P(w_1...w_m)−1
$$
本质上，PPL反映了语言模型对序列可能性预测能力的一种评估。如果$w_1...w_m$是真实的自然语言，“完美”的模型会得到$PPL = P(w_1...w_m)−1$，它对应了最低的困惑度PPL=1，这说明模型可以完美地对词序列出现的可能性进行预测。当然，真实的语言模型是无法达到PPL=1的

### 预测与搜索

给定模型结构，统计语言模型的使用可以分为两个阶段：

- 训练（Training）：从训练数据上估计出语言模型的参数。
- 预测（Prediction）：用训练好的语言模型对新输入的句子进行概率评估，或者生成新的句子。

实际上，生成最优词序列的问题也是自然语言处理中的一大类问题—序列生成（Sequence Generation）。机器翻译就是一个非常典型的序列生成任务，**序列生成任务的本质并非让语言模型凭空“生成”序列，而是使用语言模型在所有候选的单词序列中“找出”最佳序列。**这个过程对应着经典的**搜索问题**（Search Problem）

#### 搜索问题的建模

在序列生成任务中，基于语言模型的搜索问题可以被描述为：
$$
\hat{w}=argmax_{w∈χ}P(w)
$$
这里arg即argument（参数），$argmax_xf(x)$表示返回使f(x)达到最大的x。$argmax_{w∈χ}P(w) $表示找到使语言模型得分P(w)达到最大的单词序列w。χ是搜索问题的解空间，它是所有可能的单词序列w的集合。ˆ w可以被看做该搜索问题中的“最优解”，即概率最大的单词序列。

为了方便计算机实现，通常定义单词序列从一个特殊的符号<sos>后开始生成。同样地，一个单词序列的结束也用一个特殊的符号<eos>来表示,在这种序列生成策略的基础上，实现搜索通常有两种方法——**深度优先遍历**和
**宽度优先遍历**

#### 经典搜索

##### 无信息搜索

在**深度优先搜索**中，每次总是先挑选一个单词，等枚举完当前单词全部子节点构成的序列后，才会选择下一个兄弟节点继续进行搜索。

**宽度优先搜索**的过程：它维护了一个未结束单词序列的集合，每次扩展单词序列后根据长度往集合里面加入单词序列。而搜索问题关心的是单词序列的得分而非其长度。因此可以在搜索过程中维护未结束的单词序列集合里每个单词序列的得分，然后优先扩展该集合中得分最高的单词序列，使得扩展过程中未结束的单词序列集合包含的单词序列分数逐渐变高。

上面描述的两个改进后的搜索方法属于**无信息搜索**（Uninformed Search）

##### 启发性搜索

在搜索问题中，一个单词序列的生成可以分为两部分：已生成部分和未生成部分。既然最终目标是使得一个完整的单词序列得分最高，那么关注未生成部分的得分也许能为搜索策略的改进提供思路。

利用语言模型的其他特性也可以实现对未生成部分得分的估计。这个对未生成部分得分的估计通常被称为**启发式函数（Heuristic Function）**

#### 局部搜索
由于全局搜索策略要遍历整个解空间，所以它的时间、空间复杂度一般都比较高。局部搜索是非经典搜索里的一个重要方面，局部搜索策略不必遍历完整的解空间，因此降低了时间、空间复杂度，但是这也导致可能
会丢失最优解甚至找不到解，所以局部搜索都是不完备的而且非最优的。但是，在自然语言处理中，很多问题由于搜索空间过大无法使用全局搜索，因此使用局部搜索是非常普遍的。

##### 贪婪搜索

当一个问题可以拆分为多个子问题时，如果一直选择子问题的最优解就能得到原问题的最优解，那么就可以不必遍历原始的解空间，而是使用这种“贪婪”的策略进行搜索。贪婪搜索在搜索到一个完整的序列，也就是搜索到即停止，而改进的深度优先搜索会遍历整个解空间。因此贪婪搜索非常高效，其时间和空间复杂度仅为
O(m)，这里m为单词序列的长度。由于贪婪搜索并没有遍历整个解空间，所以该方法不保证一定能找到最优解。

##### 束搜索

贪婪搜索会产生质量比较差的解是由于当前单词的错误选择造成的。既然每次只挑选一个单词可能会产生错误，那么可以通过同时考虑更多候选单词来缓解这个问题，也就是对于一个位置，可以同时将其扩展到若干个节点。这样就扩大了搜索的范围，进而使得优质解被找到的概率增大。

常见的做法是每一次生成新单词的时候都挑选得分最高的前B个单词，然后扩展这B个单词的T个孩子节点，得到BT条新路径，最后保留其中得分最高的B条路径。从另外一个角度理解，它相当于比贪婪搜索看到了更多的路径，因而它更有可能找到好的解。这个方法通常被称为**束搜索（Beam Search）**。

![1718371222991](D:\software\Typora\typora-user-images\1718371222991.png)

## 词法分析和语法分析基础

![1718371550459](/C:/Users/104/AppData/Roaming/Typora/typora-user-images/1718371550459.png)

**分词（Word Segmentation）**：这个过程会把词串进行切分，切割成最小的具有完整功能的单元—单词（Word）。

**句法分析（Parsing）：**这个过程会对分词的结果进行进一步分析。比如，可以对句子进行浅层分析，得到句子中实体的信息（如人名、地名等）。也可以对句子进行更深层次的分析，得到完整的句法结构，这种结构可以被看作是对句子的进一步抽象，被称为短语结构树，比如，NP+VP就可以表示由名词短语（Noun Phrase，NP）和动词短语（Verb Phrase，VP）构成的主谓结构。

一般来说，在送入机器翻译系统前需要对文字序列进行处理和加工，这个过程被称为**预处理（Pre processing）**。类似地，在机器翻译模型输出译文后进行的处理被称作**后处理（Post processing）**。

机器翻译所使用的“单词”和“结构”本身并不是为了符合人类的解释，它们更直接目的是为了进行翻译。从系统开发的角度，有时候即使使用一些与人类的语言习惯有差别的处理，仍然会带来性能的提升，比如在神经机器翻译中，在传统分词的基础上进一步使用双字节编码（Byte Pair Encoding，BPE）子词切分 会使得机器翻译性能大幅提高。

### 中文分词

对于机器翻译系统而言，输入的是已经切分好的单词序列，而不是原始的字符串。比如，对于一个中文句子，单词之间是没有间隔的，因此需要把一个个的单词切分出来，这样机器翻译系统可以区分不同的翻译单元。

广义上，可以把上述过程看作是一种分词过程，即：将一个输入的自然语言字符串切割成单元序列，每个单元（Token）都对应可以处理的最小单位。

![1718371952606](/C:/Users/104/AppData/Roaming/Typora/typora-user-images/1718371952606.png)

#### 基于词典的分词方法

在使用基于词典的分词方法时，只需预先加载词典到计算机中，扫描输入句子，查询其中的每个词串是否出现在词典中。如图3.4所示，有一个包含六个词的词典，给定输入句子“确实现在物价很高”后，分词系统自左至右遍历输入句子的每个字，发现词串“确实”在词典中出现，说明“确实”是一个“词”。之后，重复这个过程。

![1718372159015](/C:/Users/104/AppData/Roaming/Typora/typora-user-images/1718372159015.png)

但是，基于词典的分词方法很“硬”,这是因为自然语言非常灵活，经常出现歧义，从词典中查看，“实现”和“现在”都是合法的单词，但是在句子中二者有重叠，因此词典无法告诉系统哪个结果是正确的。

![1718372341341](/C:/Users/104/AppData/Roaming/Typora/typora-user-images/1718372341341.png)

基于词典的分词方法是典型的基于规则的方法，完全依赖于人工给定的词典。在遇到歧义时，需要人工定义消除歧义的规则，比如，可以自左向右扫描每次匹配最长的单词，这是一种简单的**启发式消歧策略**。但是启发式消歧策略仍然需要人工定义

#### 基于统计的分词方法

统计分词也是一种典型的数据驱动方法。这种方法将已经经过分词的数据“喂”给系统，这个数据也被称作标注数据（Annotated Data）。

在获得标注数据后，**系统自动学习一个统计模型来描述分词的过程**，而这个模型会把分词的“知识”作为参数
保存在模型中。当送入一个新的需要分词的句子时，可以利用学习到的模型对可能的分词结果进行概率化的描述，最终选择概率最大的结果作为输出。这个方法就是基于统计的分词方法

![1718372644795](/C:/Users/104/AppData/Roaming/Typora/typora-user-images/1718372644795.png)

以“确实现在数据很多”这个实例来说，如果把这句话按照“确实/现在/数据/很/多”这样的方式进行切分，这个句子切分的概率P(确实/现在/数据/很/多)可以通过每个词出现概率相乘的方式进行计算。
$$
P(确实/现在/数据/很/多)
= P(确实)·P(现在)·P(数据)·P(很)·P(多)
$$
经过充分训练的统计模型P(·)就是我们介绍的分词模型。对于输入的新句子S，通过这个模型找到最佳的分词结果输出。假设输入句子S是“确实现在数据很多”，可以通过列举获得不同切分方式的概率，其中概率最高的切分方式，就是系统的目标输出。这种分词方法也被称作基于**1-gram语言模型的分词**，或**全概率分词**

### 命名实体识别

在翻译技术文献时，往往需要对术语进行识别并进行准确翻译，因此引入**命名实体识别（Named Entity Recognition）**可以帮助系统对特定术语进行更加细致的处理。

#### 序列标注任务

命名实体识别是一种典型的序列标注（Sequence Labeling）任务，对于一个输入序列，它会生成一个相同长度的输出序列。**输入序列的每一个位置，都有一个与之对应的输出，输出的内容是这个位置所对应的标签**（或者类别）。

通常来说，序列标注任务中首先需要定义标注策略，即使用什么样的格式来对序列进行标注。

- BIO 格式（Beginning inside outside）。以命名实体识别为例，B代表一个命名实体的开始，I表示一个命名实体的其它部分，O表示一个非命名实体单元。
- BIOES 格式。与BIO格式相比，多出了标签E（End）和S（Single）仍然以命名实体识别为例，E和S分别用于标注一个命名实体的结束位置和仅含一个单词的命名实体。

![1718373342863](/C:/Users/104/AppData/Roaming/Typora/typora-user-images/1718373342863.png)

图中的“B”、“I”、“E”等标注出了位置信息，而“CIT”和“CNT”则标注出了命名实体类别（“CIT”表示城市，“CNT”表示国家）。

#### 基于特征的统计学习

基于特征的统计学习是解决序列标注的有效方法之一。命名实体识别任务中的每个词的词根、词性和上下文组合也可以被看做是识别出命名实体可以采用的特征。

特征的形式可以分为**连续型特征**和**离散型特征**，前者通常用于表示取值蕴含数值大小关系的信息，如人的身高和体重，后者通常用于表示取值不蕴含数值大小关系的信息，如人的性别。

系统开发者可以通过定义多样的特征来从多个不同的角度对目标问题进行建模。而这种设计特征的过程也被称作特征工程（Feature Engineering）。

对于命名实体识别任务来说。输入的每个单词，可以将其表示为一个单词和对应的**词特征（Word Feature）**的组合，记作$<w,f>$。通过这样的表示，就可以将原始的单词序列转换为词特征序列。

![1718373849200](/C:/Users/104/AppData/Roaming/Typora/typora-user-images/1718373849200.png)

#### 基于概率图模型的方法

**概率图模型（Probabilistic Graphical Model**）是使用图表示**变量及变量间概率依赖关系**的方法。在概率图模型中，可以根据可观测变量推测出未知变量的条件概率分布等信息。

#### 基于分类器的方法

### 句法分析

#### 句法树

句法（Syntax）是研究句子的每个组成部分和它们之间的组合方式。自然语言处理领域最常用的两种句法分析形式是**短语结构句法分析（Phrase Structure Parsing）**和**依存句法分析（Dependency Parsing）**。

短语结构树中，每个词都有词性(或词类)，不同的词或者短语可以组成名动结构、动宾结构等语言学短语结构，短语结构句法分析一般也被称为成分句法分析（Constituency Parsing）或完全句法分析（Full Parsing）。

依存句法树表示了句子中单词和单词之间的依存关系。比如，从这个例子可以了解，“猫”依赖“喜欢”，“吃”依赖“喜欢”，“鱼”依赖“吃”。

![1718374497344](/C:/Users/104/AppData/Roaming/Typora/typora-user-images/1718374497344.png)

短语结构树和依存句法树的结构和功能有很大不同。短语结构树的叶子节点是单词，中间节点是词性或者短语句法标记。在短语结构句法分析中，通常把单词称作终结符（Terminal），把词性称为预终结符（Pre terminal），而把其他句法标记称为非终结符（Nonterminal）。

依存句法树没有预终结符和非终结符，所有的节点都是句子里的单词

#### 上下文无关文法

形式文法是分析自然语言的一种重要工具。根据乔姆斯基的定义，形式文法分为四种类型：**无限制文法（0型文法）、上下文有关文法（1型文法）、上下文无关文法（2型文法）和正规文法（3型文法）**。不同类型的文法有不同的应用，比如，正规文法可以用来描述有限状态自动机，因此也会被使用在语言模型等系统中。对于短语结构句法分析问题，常用的是上下文无关文法（Context free Grammar）

上下文无关文法的规则是一种产生式规则（Production Rule），形如α→β，它表示把规则左端的非终结符α替换为规则右端的符号序列β。通常，α被称作规则的左部（Left hand Side），β 被称作规则的右部（Right hand Side）。使用右部β 替换左

## 翻译质量评价

### 有参考答案的自动评价

人工评价费事费力，同时具有一定的主观性，甚至不同人在不同时刻面对同一篇文章的理解都会不同。如果将人类专家翻译的结果看作是参考答案，**将译文与答案的近似程度作为评价结果**。即译文与答案越接近，评价结果越好；反之，评价结果较差。这种评价方式叫做**自动评价（Automatic Evaluation）**。自动评价具有速度快，成本低、一致性高的优点，因此自动评价是也是机器翻译系统研发人员所青睐的方法。

#### 基于词串比对的方法

其思想是将译文看成是符号序列，通过计算**参考答案和机器译文间的序列相似性**来评价机器翻译的质量。

##### 基于距离的方法

基于距离的自动评价方法的基本思想是：将机器译文转化为参考答案所需要的最小编辑步骤数作为译文质量的度量，基于此类思想的自动评价方法主要有**单词错误率（Word Error Rate，WER）**、与**位置无关的单词错误率（Position independent word Error Rate，PER）** 和**翻译错误率（Translation Error Rate，TER）**

TER 是一种典型的基于距离的评价方法，通过评定机器译文的译后编辑工作量来衡量机器译文质量。在这里“距离”被定义为将一个序列转换成另一个序列所需要的最少编辑操作次数，操作次数越多，距离越大，序列之间的相似性越低；相反距离越小，表示一个句子越容易改写成另一个句子，序列之间的相似性越高。
$$
score =\frac{edit(o,g)}{l} 
$$
其中，edit(o,g) 表示系统生成的译文o和参考答案g之间的距离，l是归一化因子,通常为参考答案的长度。

例如：机器译文：A cat is standing in the ground .
			参考答案：The cat is standing on the ground .

在这个示例中一共替换了两次a->the,in->on，所以edit(o,g)=2，所以该机器译文的TER结果为2/8。

##### 基于 n-gram 的方法

`BLEU` (Bilingual Evaluation Understudy)是目前使用最广泛的自动评价指标。通过采用n-gram匹配的方式
评定机器翻译结果和参考答案之间的相似度，机器译文越接近参考答案就认定它的质量越高。`BLEU` 的计算首先考虑待评价机器译文中n-gram在参考答案中的匹配率，称**n-gram 准确率（n-gram Precision）**
$$
P_n = \frac{count_{hit}}{count_{output}}
$$
$count_{hit}$表示机器译文中n-gram在参考答案中命中的次数，$count_{output}$ 表示机器译文中总共有多少n-gram

为了避免同一个词被重复计算，`BLEU`的定义中使用了截断的方式定义$count_{hit}$和$count_{output}$ 。

令N表示最大n-gram的大小，则译文整体的准确率等于各n-gram的加权平均：
$$
P_{avg} = exp(\sum^{N}_{n=1}w_n·logP_n)
$$
该方法更倾向于对短句子打出更高的分数。一个极端的例子是译文只有很少的几个词，但是都命中答案，准确率很高可显然不是好的译文。因此，`BLEU`引入短句惩罚因子（Brevity Penalty，BP）的概念，对短句进行惩罚:
$$
BP = \begin{cases}
1 &\  c>r\\
exp(1− \frac{r}{c})&\ c≤r

\end{cases}
$$
c表示机器译文的句子长度，r表示参考答案的句子长度。最终`BLEU`的计算公式为：
$$
BLEU = BP·exp(\sum^{N}_{N=1}w_n·logP_n)
$$
实际上，`BLEU`的计算也是一种综合考虑**准确率（Precision）**和**召回率（Recall）**的方法。BP是一种召回率的度量，它会惩罚过短的结果；$exp(\sum^{N}_{N=1}w_n·logP_n)$是一种准确率的表示

准确率（Precision）和召回率（Recall）是评估分类模型性能的两个重要指标，它们通常用于信息检索和机器学习领域。

**准确率（Precision）**：在所有被模型预测为正类（positive class）的样本中，实际为正类的比例。

**召回率（Recall）**：在所有实际为正类的样本中，被模型正确预测为正类的比例。

虽然`BLEU`被广泛使用，但也并不完美，甚至经常被人诟病。比如，它需要依赖参考答案，而且评价结果有时与人工评价不一致，同时`BLEU`评价只是单纯地从词串匹配的角度思考翻译质量的好坏，并没有真正考虑句子的语义是否翻译正确。但是，毫无疑问，`BLEU`仍然是机器翻译中最常用的评价方法。在没有找到更好的替代方案之前，`BLEU`还是机器翻译研究中最重要的评价指标之一。

#### 基于词对齐的方法

基于词对齐的方法，顾名思义就是根据参考答案中的单词与译文中的单词之间的对齐关系对机器翻译译文进行评价。

在基于词对齐的自动评价方法中，一种典型的方法是Meteor。该方法通过计算精确的单词到单词（Word to Word）的匹配来度量一个译文的质量，并且在精确匹配之外，还引入了“波特词干”匹配和“同义词”匹配。

在机器译文与参考答案之间建立单词之间的对应关系。单词之间的对应关系在建立过程中主要涉及三个模型，在对齐过程中依次使用这三个模型进行匹配：

首先是精确模型:精确模型在建立单词对应关系时，要求机器译文端的单词与参考答案端的单词完全一致，并且在参考答案端至多有1个单词与机器译文端的单词对应，否则会将其视为多种对应情况。

![1718429996315](/C:/Users/104/AppData/Roaming/Typora/typora-user-images/1718429996315.png)

然后是波特词干模型，该模型在精确匹配结果的基础上，对尚未对齐的单词进行基于词干的匹配，只需机器译文端单词与参考答案端单词的词干相同即可，如上文中的“he”和“him”

![1718430049901](/C:/Users/104/AppData/Roaming/Typora/typora-user-images/1718430049901.png)

最后是同义词模型该模型在前两个模型匹配结果的基础上，对尚未对齐的单词进行同义词的匹配，即基于Word Net词典匹配机器译文与参考答案中的同义词。如图中的“eat”和“have”

![1718430123813](/C:/Users/104/AppData/Roaming/Typora/typora-user-images/1718430123813.png)

由于召回率反映参考答案在何种程度上覆盖目标译文的全部内容，而Meteor在评价过程中显式引入召回率，所以Meteor的评价与人工评价更为接近。但Meteor方法需要借助同义词表、功能词表等外部数据，当外部数据中的目标词对应不正确或缺失相应的目标词时，评价水准就会降低。特别是，针对汉语等与英语差异较大的
语言，使用Meteor方法也会面临很多挑战。不仅如此，超参数的设置和使用，对于评分也有较大影响。

#### 基于检测点的方法

上述提到的方法可以对译文的整体质量进行评估，但是缺乏对具体问题的细致评价。基于检测点的方法可以让研究人员需要知道系统是否能够处理特定类型的翻译问题，而不是得到一个笼统的评价结果。

基于检测点的评价根据事先定义好的语言学检测点对译文的相应部分进行打分。如下是几个英中翻译中的检测点实例：

- They got up at six this morning .
  他们/今天/早晨/六点钟/起床/。
  检测点：时间词的顺序

- There are nine cows on the farm .

  农场/里/有/九/头/牛/。
  检测点：量词“头”

- His house is on the south bank of the river .
  他/的/房子/在/河/的/南岸/。
  We keep our money in a bank .
  我们/在/一家/银行/存钱/。
  检测点：bank的多义翻译

基于检测点的评价方法的意义在于，它并不是简单给出一个分数，反而更像是一种诊断型评估方法，能够帮助系统研发人员定位系统问题。因此这类方法更多地使用在对机器翻译系统的翻译能力进行分析上，是对BLEU等整体评价指标的一种很好的补充。

#### 多策略融合的评价方法

基于策略融合的自动评价方法往往会将多个基于词汇、句法和语义的自动评价方法融合在内，其中比较核心的问题是如何将多个评价方法进行合理地组合。目前提出的方法中颇具代表性的是使用参数化方式和非参数化方式对多种自动评价方法进行筛选和组合。

参数化组合方法的实现主要有两种方式：

- 一种方式是广泛使用不同的译文质量评价作为特征，借助回归算法实现多种评价策略的融合；
- 另一种方式则是对各种译文质量评价方法的结果进行加权求和，并借助机器学习算法更新内部的权重参数，从而实现多种评价策略的融合

#### 译文多样性

由于句子的灵活排序和大量同义词的存在，导致同一个源语言句子可能对应几百个合理的目标语言译文，甚至更多。为了改变这种窘况，比较直观的想法是增大参考答案集或是直接比较机器译文与参考答案在词法、句法和语义等方面的差距。



### 无参考答案的自动评价

## 人工神经网络和神经语言建模





















