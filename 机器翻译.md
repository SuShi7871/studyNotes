# 机器翻译

## fairseq

Fairseq 是一个用于序列建模任务的工具库，特别是在机器翻译、文本生成和语言建模等领域中广泛使用。它基于PyTorch，并提供了一系列预训练模型和训练、评估以及推理的工具。

### fairseq的安装

```bash
git clone https://github.com/pytorch/fairseq
cd fairseq
pip install --editable ./
```

### fairseq的使用

当使用Fairseq进行中文数据的预处理时，可以参考以下命令和参数说明：

```bash
# Fairseq提供的命令行工具，用于数据预处理。
fairseq-preprocess --source-lang zh --target-lang en \
	# 指定源语言为中文（'zh'),指定目标语言为英文（'en'）。
    --trainpref $PATH_TO_DATA/train.zh-en \
    # 指定训练数据文件的前缀。
    --validpref $PATH_TO_DATA/valid.zh-en \
    # 指定验证数据文件的前缀
    --testpref $PATH_TO_DATA/test.zh-en \
    # 指定测试数据文件的前缀
    --destdir data-bin/tokenized.zh.en \
    # 指定预处理过程中使用的工作线程数。根据您系统的性能调整此参数，以获得更快的预处理速度。
    --workers 10 \
    # 可选参数，表示在评估或测试过程中使用BLEU分数进行评分。
    --scoring bleu
```

使用方法:

1. **准备数据**: 确保您的数据文件位于指定路径，并按照需要进行预处理（例如分词和编码）。

2. **运行命令**

3. **执行命令**: 执行命令以进行数据预处理。Fairseq将会对数据进行分词、处理，并将其准备成适合Fairseq模型训练和评估的格式。

这个命令根据您的硬件配置，调整`--workers`等参数以获得最佳性能。预处理完成后，`data-bin/tokenized.zh.en`目录将包含预处理后的数据，准备好用于Fairseq的模型训练。

```bash
# 指定训练数据的位置，这里是经过预处理的藏文（bo）和英文（en）数据目录。
fairseq-train ./data-bin/tokenized.bo.en.bpe32k/ \
# 训练的最大epoch数，即完整遍历训练数据的次数。
    --max-epoch 50 \
    # 指定分布式数据并行的后端，这里是禁用c10d，表示不使用PyTorch的分布式数据并行。
    --ddp-backend=no_c10d \
    # 使用Transformer模型架构进行训练
    --arch transformer \
    # 在Transformer模型中，共享解码器的输入和输出embedding。
    --share-decoder-input-output-embed \
    # 化器选择Adam，设置Adam优化器的参数beta1和beta2。
    --optimizer adam --adam-betas '(0.9, 0.98)' \
    # 初始学习率为5e-7。学习率调度器选择inverse_sqrt，即按照时间的倒数平方根调整学习率。
    --lr 5e-7 --lr-scheduler inverse_sqrt \
    # warmup期间的初始学习率，即学习率从较小的值（1e-07）开始逐渐增加。
    --warmup-updates 4000 --warmup-init-lr '1e-07' \
    # 标签平滑的参数，用于减少过拟合。
    # 损失函数选择label_smoothed_cross_entropy，结合标签平滑和交叉熵损失。
    --label-smoothing 0.1 --criterion label_smoothed_cross_entropy \
    # 模型中的dropout比例为0.3，用于减少过拟合 权重衰减项的系数，用于控制模型的正则化强度。
    --dropout 0.3 --weight-decay 0.0001 \
    # 指定保存训练模型和日志的目录。
    --save-dir checkpoints \
    # 每个batch中最大的token数量。
    --max-tokens 900 \
    # 累计梯度的步数，即每几个batch更新一次模型参数。
    --update-freq 8 \
    # TensorBoard日志目录，用于可视化训练过程。
    --tensorboard-logdir tensor \
    # 评估指标选择BLEU分数，用于在训练期间评估模型性能。
    --scoring bleu	
```

这是一个用于训练神经机器翻译模型的命令，下面是各个参数的解释：

## 复现Tibetan To Englsih Machine Translation

### 1.使用fairseq进行复现

首先是进入到preprocessing里面进行数据预处理，准备和处理机器翻译任务中的训练、验证和测试数据。主要功能可以总结如下：

- 准备和划分数据集（训练、验证、测试）。
- 学习联合BPE模型以进行子词编码。
- 对数据集进行BPE编码以生成处理后的训练、验证和测试数据。

然后是执行fairseq的预处理指令

```bash
fairseq-preprocess --source-lang bo --target-lang en \
				--trainpref /mnt/workspace/tibetToenglish/Bo-Eng-Machine-			Transation/Fairseq/preProcessing/data.bo.en.bpe16k/train.bpe.bo-en \
				--validpref /mnt/workspace/tibetToenglish/Bo-Eng-Machine-Transation/Fairseq/preProcessing/data.bo.en.bpe16k/valid.bpe.bo-en \
				--testpref  /mnt/workspace/tibetToenglish/Bo-Eng-Machine-Transation/Fairseq/preProcessing/data.bo.en.bpe16k/test.bpe.bo-en \
				--destdir /mnt/workspace/tibetToenglish/Bo-Eng-Machine-Transation/Fairseq/preProcessing/data.bo.en.bpe16k/data-bin/tokenized.bo.en.bpe32k \
				--workers 10 \
				--scoring bleu
```

处理后的数据将包括词汇表和数据集的二进制文件，准备好供 fairseq 训练和评估使用。

```bash
fairseq-train /mnt/workspace/tibetToenglish/Bo-Eng-Machine-Transation/Fairseq/preProcessing/data.bo.en.bpe16k/data-bin/tokenized.bo.en.bpe32k/ \
    --max-epoch 50 \
    --ddp-backend=no_c10d \
    --arch transformer \
    --share-decoder-input-output-embed \
    --optimizer adam --adam-betas '(0.9, 0.98)' \
    --lr 5e-7 --lr-scheduler inverse_sqrt \
    --warmup-updates 4000 --warmup-init-lr '1e-07' \
    --label-smoothing 0.1 --criterion label_smoothed_cross_entropy \
    --dropout 0.3 --weight-decay 0.0001 \
    --save-dir checkpoints \
    --max-tokens 900 \
    --update-freq 8 \
    --tensorboard-logdir tensor \
    --scoring bleu
```

在下一步就是对上述的文件进行进一步训练，生成训练日志和模型检查点。训练完成后，可以使用保存的模型检查点进行推理或进一步微调。模型文件最后的存放路径为fairseq-train 

```cmd
/mnt/workspace/checkpionts
```





































