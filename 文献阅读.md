### 文献阅读

## NLP

### ``3AM``: An Ambiguity-Aware Multi-Modal Machine Translation Dataset

#### 简介

`3AM``：歧义感知多模态机器翻译数据集

现有 `MMT `数据集提供的视觉信息不足，导致模型忽视它并高估了其能力。本文通过引入` `3AM` `来解决这个问题，这是一个歧义感知 `MMT `数据集，包含 26,000 个平行的英语和中文句子对，每个句子都有相应的图像。

数据、代码和脚本可在 https://github.com/MaxyLee/`3AM`

`3AM` 将迫使 `MMT `模型优先考虑视觉信息。 当句子中的单词不明确时，解决其含义的唯一方法是参考图像。 因此，有人认为 `3AM` 数据集将阻止 `MMT `模型仅依靠语言先验来实现高性能。 该数据集有望促进 `MMT `评估，更准确地反映模型理解视觉信息的能力。 为了验证这一假设，我们根据 `3AM` 和其他现有 `MMT `数据集评估了许多 `MMT `模型。 在纯文本模型和多模态模型上进行了大量的实验。

本文的三个主要贡献：

1. 我们提出了一个 `MMT `数据集，该数据集专门设计用于通过收集各种模棱两可的句子来提高对视觉信息的理解。与现有的 `MMT `数据集相比，`3AM`数据集更具挑战性，包含更丰富的概念集;
2. 我们在我们提出的数据集上评估了最先进的 `MMT `模型的性能，并表明可以利用视觉信息的模型优于纯文本模型。这一发现支持了我们的假设，即 `3AM` 数据集可以鼓励 `MMT `模型更好地利用视觉信息并改善翻译结果;
3. 我们构建数据集的方法涉及收集模棱两可的数据，这些数据也可用于其他多模态学习数据集

![1715846832087](C:\Users\104\AppData\Roaming\Typora\typora-user-images\1715846832087.png)

#### 实现

##### 1.数据源

Considering that a large portion of the source data is noisy, we filter the data to ensure quality,
which is divided into two steps. The first step is a rule-based filtering approach:

第一步是基于规则的筛选方法：

- 句子长度过滤
- `url`过滤:由于同一URL前缀下的图片通常属于同一类型，因此我们汇总了图片质量较差的URL的集合，并过滤掉了相应的数据。
- 关键字过滤。我们删除了包含某些关键字的句子，这些关键字通常质量很差。
- 专有名词过滤

第二步是语言模型过滤，我们使用语言模型对文本进行评分并过滤掉质量较差的字幕。

主要是通过`GPT2`过滤掉质量较差的字幕,指定字幕T的得分如下，分数越高效果越差:
$$
GPTScore = exp(GPT(T ))
$$

##### 2.数据选择

1.数据过滤

- 第一步是利用现有的`WSD`数据集构建词义词典，在此基础上可以获取包含歧义词的数据。
- 第二步建立了一个包含模棱两可的单词及其中文翻译和相应定义的词典。使用`BabelNet`来过滤掉没有歧义的单词
- 第三步对有歧义的单词进行选择有歧义的句子，请注意，一个句子可能包含多个模棱两可的单词，它们在下一步中被视为不同的数据条目。

2.词义消歧（Word Sense Disambiguation）

使用自动办法来判断词是否在上下文中有歧义，这里使用`WSD` 模型，对上一步获得的数据进行评分。给定一个句子 T 和一个目标词 w，上下文中目标词的一组感觉 S 的概率分布 P 可以定义为：
$$
P (S|T, w) = WSD(T, w, S)
$$
按照`Pasini`和`Navigli`（2017）的说法，句子T中单词w的歧义得分可以定义为：
$$
AmbigScore(T, w) = P (s1|T, w) − P (s2|T, w)
$$
其中 $s= argmaxP （s|T， w） $和$ s= argmaxP （s|T，w）$然后，我们使用歧义分数按降序对数据进行排序，并获得` WSD `模型认为最有可能模棱两可的数据的排名列表。

3.数据标注

按照以上要求进行数据标注，总共对大约 26000 个数据项进行了注释，我们手动选择了 1000 个高度模糊的句子以包含在我们的验证和测试集中。

##### 3.数据集统计信息

总结下来就是`3AM`数据集好

##### 4.实验

###### 数据集选择

对于数据集，除了`3AM`的实验外，我们还在其他 `MMT `数据集上进行了实验进行比较

- `Multi30K`

  是基于`Flickr30K`的常用`MMT`数据集

- `MSCTD`

   是一个多模态情感聊天翻译数据集，包含 142,871 个英汉平行句和 30,370 个英德平行句。我们使用`MSCTD`的英汉翻译数据与我们的数据集进行比较。

###### 基准测试

为了给`3AM提`供一个令人信服的基准测试，我们以多个机器翻译模型为基准模型进行了实验,可分为两类：纯文本模型和多模态模型

纯文本模型

- Trans

  ​       Transformer

- Bart. 

  ​     是一个基于 Transformer 的模型，通过使用任意噪声破坏文本进行预训练，并允许模型重建它。

- `T5.`

  ​    是一种 Transformer 模型，在有监督和无监督任务的混合上进行了预训练，这些任务被转换为文本到  	文本格式

多模态模型

- `SelAttn`

  ​    选择性注意力是一种`MMT`模型，它使用由基于Transformer的模型(如视觉转换器(`ViT`))提取的视觉特  	征

- `VL-Bart`

  ​    是一种多模态预训练模型，它通过将图像特征作为附加输入，将 Bart 模型编码器扩展到多模态编码器。

- `VL-T5`

  ​    是一个多模态预训练模型，使用与` VL-Bart` 相同的方法从 `T5` 扩展而来。

##### 5.实现细节

###### 训练

模型基于`PyTorch`、`Fairseq`和 `Huggingface Transformers`实现

硬件环境：`Nvidia A40 GPU `

1. 从头开始训练的模型。Transformer 和 Selective Attention 模型从头开始训练，由 4 个编码器和解码器层组成。我们将隐藏大小设置为 128，将 FFN 的滤波器大小设置为 256，在多头自注意力中设置 4 个头。我们使用 Adam Optimizer，β= .9、β= .98 和 ε = 10。我们以 4096 个令牌的批量大小和 6 × 10 的学习率训练模型。
2. 
3. 预训练模型。我们使用 Huggingface Transformer 的预训练 T5-bases 和 Bart-basemodel 按照 Cho 等人 (2021) 进行微调。这些模型经过微调，批量大小为 60，学习率为 5 × 10。我们使用 AdamW 优化，权重衰减为 0.01

###### 评估

使用BLEU、BERT-Score、METEOR和TER作为自动评估指标。

#### 结论

实验结果如表3所示。每个模型都在三个数据集上进行训练，并在每个测试集上评估性能。尽管在 `Multi30K` 上训练的`MMT` 模型在 `Multi30K `和 `MSCTD` 测试集中的表现接近甚至更差，但它们在 `3AM `测试集中的表现略好。这一结果证实了我们的观点，即尽管当前的`MMT`模型可以利用视觉信息，但由于视觉信息的作用很小，这种能力很难反映在现有的数据集中。此外，与纯文本模型相比，在` MSCTD` 数据集上训练的` MMT` 模型性能较差，这表明 `MSCTD `无法让模型学习如何利用视觉信息。此外，在 `MSCTD` 数据集上训练的模型在其他测试集上表现不佳，因为它是对话数据集，与其他数据有很大不同。此外，在 `3AM` 数据集上训练的` MMT` 模型的性能远远优于纯文本模型，这表明视觉信息在我们的 `3AM` 数据集中起着至关重要的作用。这些发现证实了我们的假设，即我们提出的数据集将迫使`MMT`模型利用视觉信息来消除歧义，从而产生更高质量的翻译

![1715851354985](C:\Users\104\AppData\Roaming\Typora\typora-user-images\1715851354985.png)

### Vocabulary Learning via Optimal Transport  for Neural Machine Translation

#### 简介

为了找到大小合适的最佳标记词典，作者提出了 VOLT，一种简单有效的解决方案，无需试用培训。实证结果表明，VOLT在WMT-14英德和TED多语言翻译等多种场景中均优于广泛使用的词汇。

https： //github.com/Jingjing-NLP/VOLT

由于文本的离散性，词汇结构（简称词汇化）是神经机器翻译 （NMT）任务的先决条件，目前，字节对编码 （BPE） 等子词方法被广泛使用

BPE主要是考虑了选取最常见的子词（或概率更高的词段）作为词汇标记，这种方式确实使得生成的语料库易于学习和预测，但是，他没有考虑到词汇量的问题，因为词汇量的大小也会影响到下游的表现，尤其是在资源不足的任务上。没考虑词汇量大小的后果就是为了选择最佳的大小，往往需要做大量的实验，目前，为了方便起见，同意设置固定的尺寸，例如:30K-40K;

作者提到的VOLT是同时考虑熵和词汇量来探索自动词汇化，语料库熵随着词汇量的增加而减少，这有利于模型学习，另一方面，太多的标记会导致标记稀疏，这损害了模型学习

借用经济学中的概念边际效应，为了平衡熵（收益）和词汇量（成本）的关系，作者提出了词汇化边际效用（MUV）作为衡量标准,预计更高的 MUV 可实现帕累托最优性。

#### 相关工作

##### 词汇化的理解

在机器翻译（Machine Translation, MT）中，"词汇化"（Tokenization）是指将文本分解成更小的单元，这些单元可以是单词、短语、子词（subword）或其他语言元素的过程。词汇化是机器翻译中的一个关键步骤，因为它影响着模型对输入文本的理解和处理方式。

词汇化的主要目的是：

1. **标准化文本**：将不同形式的文本统一成模型易于处理的格式。例如，将所有英文文本转换为小写，以消除大小写的差异。

2. **分词**：将句子分解成单词或更小的单位。在某些语言中，如中文和日文，分词尤为重要，因为它们没有明显的单词边界。

3. **处理未知词汇**：通过将词汇分解成更小的单元，模型可以更好地处理未见过的词汇或罕见词汇。

4. **提升模型的泛化能力**：通过词汇化，模型可以学习到词汇的子单元（如词根、词缀）之间的关系，从而提高对新词和新短语的翻译能力。

在机器翻译的不同阶段，词汇化的方法可能有所不同：

- **传统机器翻译**：通常使用规则或统计方法进行分词，将文本分解成单词或短语。

- **神经机器翻译（NMT）**：随着深度学习技术的发展，NMT模型开始采用子词（subword）分割策略，如Byte Pair Encoding（BPE）、WordPiece或SentencePiece等。这些方法可以自动学习词汇的最佳分割方式，以适应不同的语言和词汇特性。

词汇化对于机器翻译系统的性能至关重要，因为它直接影响到模型的输入表示和翻译质量。通过有效的词汇化策略，机器翻译系统能够更好地理解和生成自然、流畅的翻译结果。

单词级词汇无法在有限的词汇量下处理生僻单词。

常见的词汇化方法

byte level approaches、 character level approaches、sub-word approaches

Byte-Pair Encoding (BPE)，他是一种基于子词级词汇表的方法，主要做法是通过合并成对的频繁字符序列以创建子词单元，子词词汇可以看作是字符级词汇和词级词汇之间的权衡。与词级词汇相比，它可以降低标记的稀疏性，增加相似词之间的共享特征，这些词可能具有相似的语义含义，比如说：“happy” 和 “happier”.

上述的词汇化方法虽然有效，但是忽略了一个问题，就是他们没有考虑到词汇量的大小

##### MUV

Spearman scores：主要是用来度量两个变量之间相关性的非参数统计方法。斯皮尔曼等级相关系数是通过分析变量的等级（或排序）来确定它们之间是否存在相关性

斯皮尔曼等级相关系数用字母ρ（rho）表示，其值的范围在-1到+1之间：

- **+1** 表示完美的正相关，即一个变量的等级增加时，另一个变量的等级也相应增加。
- **-1** 表示完美的负相关，即一个变量的等级增加时，另一个变量的等级相应减少。
- **0** 表示没有相关性，即两个变量的等级之间没有明显的关联。

![1716706401209](C:\Users\104\AppData\Roaming\Typora\typora-user-images\1716706401209.png)

MUV 的定义 从形式上讲，MUV 表示熵对大小的负推导。为了简化起见，我们利用较小的词汇来估计实现中的 MUV。具体而言，MUV的计算公式为：
$$
M_{v(k+m)} = \frac{-(H_{v(k+m)}- H_{v(k)})}{m}
$$
v(k)代表词汇表，里面有k个标记，$H_v$代表语料库的熵

为了避免标记的长度太长，我们需要对熵进行归一化，最终的熵定义为：
$$
H_v =-\frac{1}{l_v}\sum_{j \epsilon v}P(j)logP(j)
$$
lv是词汇表中标记的平均长度， $P（j）$ 是训练语料库中标记 j 的相对频率

基于以上分析，我们有两种方案来获得最后的词汇表：搜索和学习

首先是搜索，我们可以基于BPE来遍历所有候选词汇来获得最佳词汇表，虽然他简单有效，但是生成词汇表和计算 MUV 仍然需要大量时间，所以作者提出了一种基于学习的方案来获得最后的词汇表，也就是之前提出的VOLT方法

长尾分布

argmax 

argmax 是一个常用操作，用来找到某个函数在定义域内取得最大值时的点。具体来说，对于函数 𝑓(𝑥)，argmax 𝑓(𝑥) 表示使得 𝑓(𝑥)达到最大值的 𝑥的值。

在机器学习和深度学习中，argmax操作经常用于：

1. **分类问题**：在多分类问题中，模型的输出层通常会输出每个类别的得分（例如，通过softmax函数）。使用 argmax 可以确定哪个类别的得分最高，即模型预测为该类别。
2. **优化问题**：在训练神经网络时，我们通常希望找到一组参数，使得模型的性能指标（如损失函数）最小化。虽然我们通常使用梯度下降等方法来最小化损失，但 argmax可以用来找到最大化某个目标函数的参数。
3. **特征选择**：在特征选择中，argmax 可以用来选择那些对模型性能提升最大的特征。
4. **注意力机制**：在序列模型中，如Transformer，argmax可以用来确定注意力权重的最大值，即模型最关注的输入序列中的哪个部分。

##### VOLT



### 基于图文交互增强低资源神经机器翻译方法研究

#### 摘要

低资源语言翻译的难点在于可用资源较少， 在己有低资源翻译方法中， 并没有充分的使用图像这
可以跨越语言壁垒的资源， 因此本文立足于图像信息可以拉近不同语言信息并可以补充文本信息没
有的信息这特点将图像信息用图文检索,多模态门控信息增强等方式融入低资源神经机器翻译的任务中

1. 图文数据细粒度检索及预处理：
2. 基于图文多模态门控增强的文本平行句对抽取方法研究：



### 机器翻译中的幻觉

`https://arxiv.org/pdf/2406.07239`

在机器翻译中，幻觉（hallucination）指的是生成的翻译输出中包含了在源文本中不存在的信息。简单来说，机器翻译模型在没有依据源文本内容的情况下，生成了一些额外的、不正确的内容。幻觉会导致翻译结果看起来流畅、合乎语法，但却与原文的实际意义不符或包含完全捏造的信息。

特别是在同声机器翻译（SiMT）中，已经发现幻觉是极其严重的

作者发现当SiMT模型更多地关注目标侧信息时，幻觉确实明显更严重。利用这一点，我们通过在目标侧上下文中引入噪声来减少过度目标依赖效应。实验结果表明，当延迟相对较小时，所提出的方法在蓝色和幻觉效果方面实现了一些适度的改进。这一发现给了我们一些启示：对目标侧信息的使用进行更灵活的控制可能是缓解幻觉问题的一种有希望的方法。

作者研究发现Wait-k模型比全句MT更容易产生幻觉。此外，随着k的降低，幻觉明显增加。



Hallucination words are with high distribution entropy.











